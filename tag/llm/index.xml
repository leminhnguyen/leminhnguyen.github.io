<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm | leminhnguyen&#39;s blog</title>
    <link>https://leminhnguyen.github.io/tag/llm/</link>
      <atom:link href="https://leminhnguyen.github.io/tag/llm/index.xml" rel="self" type="application/rss+xml" />
    <description>llm</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© {2025} leminhnguyen</copyright><lastBuildDate>Sat, 11 Jan 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>llm</title>
      <link>https://leminhnguyen.github.io/tag/llm/</link>
    </image>
    
    <item>
      <title>Adversarial Attacks on Large Language Models (LLMs)</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</link>
      <pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Large Language Models (LLMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Adversarial attacks on large language models (LLMs) involve manipulating inputs to deceive the model into generating harmful, biased, or incorrect outputs. These attacks exploit the vulnerabilities of LLMs, which rely on patterns in training data to generate responses. Below is an overview of key concepts, types of attacks, implications, and defense strategies.&lt;/p&gt;














&lt;figure  id=&#34;figure-an-overview-of-threats-to-llm-based-applications-source-lillog-blog&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An overview of threats to LLM-based applications (source: Lil&amp;#39;Log Blog)&#34; srcset=&#34;
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png 400w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_39113d4dab022780f9de83ba4af61166.png 760w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png&#34;
               width=&#34;760&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An overview of threats to LLM-based applications (source: Lil&amp;rsquo;Log Blog)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-types-of-adversarial-attacks-on-llms&#34;&gt;&lt;strong&gt;1. Types of Adversarial Attacks on LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-evasion-attacks&#34;&gt;a. Evasion Attacks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Attackers modify input text (e.g., by altering words, punctuation, or structure) to trick the model into producing unintended outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding a few words like &amp;ldquo;I am a helpful assistant&amp;rdquo; to a prompt to manipulate the model&amp;rsquo;s response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Can lead to misinformation, phishing, or generation of harmful content.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-poisoning-attacks&#34;&gt;&lt;strong&gt;b. Poisoning Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Corrupting training data to influence the model&amp;rsquo;s behavior. Attackers inject malicious examples during training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Including biased or harmful data in the training set to make the model generate toxic responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Long-term degradation of model reliability and trustworthiness.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-injection-attacks&#34;&gt;&lt;strong&gt;c. Injection Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inserting malicious code or prompts into the input to alter the model&amp;rsquo;s execution flow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Using adversarial prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; to exploit the model&amp;rsquo;s tendency to follow instructions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Enables exploitation of model capabilities for malicious purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-data-poisoning&#34;&gt;&lt;strong&gt;d. Data Poisoning&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Similar to poisoning attacks but focuses on corrupting the training dataset to bias the model&amp;rsquo;s outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding fake user interactions that encourage the model to generate harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Systemic bias and ethical risks in model behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-model-inversion-attacks&#34;&gt;&lt;strong&gt;e. Model Inversion Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inferring sensitive information about the model&amp;rsquo;s training data by analyzing outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Reverse-engineering the model to reveal private data or patterns in the training set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Privacy breaches and exposure of proprietary information.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-implications-of-adversarial-attacks&#34;&gt;&lt;strong&gt;2. Implications of Adversarial Attacks&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security Risks&lt;/strong&gt;: Phishing, misinformation, and malware generation via manipulated prompts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ethical Concerns&lt;/strong&gt;: Reinforcement of biases, hate speech, or harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trust Erosion&lt;/strong&gt;: Users may lose confidence in LLMs for critical tasks like healthcare, finance, or legal advice.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operational Disruption&lt;/strong&gt;: Attackers could disrupt services by causing models to fail or produce incorrect outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-defense-mechanisms&#34;&gt;&lt;strong&gt;3. Defense Mechanisms&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-adversarial-training&#34;&gt;&lt;strong&gt;a. Adversarial Training&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models on adversarial examples to improve robustness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Introduce perturbed inputs during training to make the model resistant to attacks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires access to adversarial examples, which may be difficult to generate for LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-input-sanitization&#34;&gt;&lt;strong&gt;b. Input Sanitization&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Detect and filter malicious patterns in inputs (e.g., using regex or keyword matching).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Blocking suspicious prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; or &amp;ldquo;I am a helpful assistant.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May fail against sophisticated, subtle attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-model-ensembles&#34;&gt;&lt;strong&gt;c. Model Ensembles&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Use multiple models to cross-validate outputs and detect inconsistencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If one model generates a harmful response, others may flag it as anomalous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Increases computational overhead and complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-uncertainty-estimation&#34;&gt;&lt;strong&gt;d. Uncertainty Estimation&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models to estimate confidence in their outputs, flagging uncertain responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If the model is unsure about a prompt, it may refuse to generate a response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires careful calibration and may reduce usability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-prompt-engineering-defenses&#34;&gt;&lt;strong&gt;e. Prompt Engineering Defenses&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Design prompts to resist adversarial manipulation (e.g., using multi-step reasoning or safety checks).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Incorporating safety constraints like &amp;ldquo;Avoid harmful content&amp;rdquo; into the prompt.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May not fully prevent attacks, especially if the adversary tailors prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-research-and-tools&#34;&gt;&lt;strong&gt;4. Research and Tools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Papers&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Adversarial Examples for Neural Network Language Models&amp;rdquo;&lt;/strong&gt; (Emti et al.) â€“ Explores adversarial examples in NLP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Prompt Injection Attacks on Language Models&amp;rdquo;&lt;/strong&gt; (Zhang et al.) â€“ Demonstrates how prompts can be weaponized.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Defending Against Prompt Injection Attacks&amp;rdquo;&lt;/strong&gt; (Li et al.) â€“ Proposes defenses against adversarial prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adversarial Text Generation Tools&lt;/strong&gt;: Generate adversarial examples for testing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Auditing Frameworks&lt;/strong&gt;: Analyze model behavior for biases or vulnerabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-challenges-and-future-directions&#34;&gt;&lt;strong&gt;5. Challenges and Future Directions&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Nature of Attacks&lt;/strong&gt;: Adversaries continuously evolve techniques, requiring ongoing research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balancing Safety and Usability&lt;/strong&gt;: Defenses must avoid overly restrictive measures that hinder model functionality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Domain Collaboration&lt;/strong&gt;: Combining insights from cybersecurity, NLP, and ethics to address risks holistically.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Adversarial attacks on LLMs pose significant risks to security, ethics, and trust. While defenses like adversarial training and input sanitization offer partial protection, the dynamic nature of these threats demands continuous innovation. Researchers and practitioners must prioritize robustness, transparency, and ethical considerations to ensure the safe deployment of LLMs in real-world applications.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GLiNER: A Generalist Model for Named Entity Recognition using Bidirectional Transformers</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/gliner/</link>
      <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/gliner/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h3 id=&#34;1-what-is-named-entity-recognition-ner&#34;&gt;1. What is Named Entity Recognition (NER)?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; is a fundamental task in Natural Language Processing (NLP) that involves &lt;strong&gt;identifying and classifying spans of text&lt;/strong&gt; that refer to real-world entities such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Persons&lt;/strong&gt; (e.g., &amp;ldquo;Albert Einstein&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organizations&lt;/strong&gt; (e.g., &amp;ldquo;United Nations&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Locations&lt;/strong&gt; (e.g., &amp;ldquo;Paris&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dates, Products, Diseases&lt;/strong&gt;, and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional NER systems are trained on a &lt;strong&gt;fixed set of entity types&lt;/strong&gt;, which limits their adaptability to new domains or tasks. Recently, &lt;strong&gt;Open NER&lt;/strong&gt; has emerged as a flexible paradigm that allows recognizing arbitrary entity types based on natural language instructions â€” a direction GLiNER directly embraces and enhances.&lt;/p&gt;
&lt;h3 id=&#34;2-overview&#34;&gt;2. Overview&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GLiNER&lt;/strong&gt; is a compact and general-purpose model for &lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; that leverages &lt;strong&gt;Bidirectional Transformers&lt;/strong&gt; (like BERT or DeBERTa) to extract arbitrary types of entities from text â€” without being constrained to a fixed label set. Unlike traditional NER models or large language models (LLMs) like ChatGPT, GLiNER is &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;efficient&lt;/strong&gt;, and designed for &lt;strong&gt;zero-shot generalization&lt;/strong&gt; across domains and languages.&lt;/p&gt;
&lt;p&gt;Traditional NER systems are limited by a fixed ontology of entity types. While LLMs (e.g., GPT-3, ChatGPT) allow open-type NER via prompting, they are &lt;strong&gt;computationally expensive&lt;/strong&gt;, &lt;strong&gt;slow (token-by-token decoding)&lt;/strong&gt;, and often impractical in production due to API cost and latency. GLiNER aims to:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: -15px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Retain the &lt;strong&gt;flexibility of LLMs&lt;/strong&gt; in handling arbitrary entity types.&lt;/li&gt;
&lt;li&gt;Achieve &lt;strong&gt;high performance&lt;/strong&gt; with &lt;strong&gt;orders of magnitude fewer parameters&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Enable &lt;strong&gt;parallel extraction&lt;/strong&gt; of entities rather than autoregressive generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id=&#34;3-model-architecture&#34;&gt;3. Model Architecture&lt;/h3&gt;
&lt;p&gt;GLiNER reframes NER as a &lt;strong&gt;semantic matching problem&lt;/strong&gt; between entity types and text spans in a &lt;strong&gt;shared latent space&lt;/strong&gt;.














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png 400w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_b31f86d47083376b16f5847cefd3ac4d.png 760w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Input Format&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ENT] person [ENT] organization [ENT] location [SEP] Text...
- `[ENT]`: special token preceding each entity type.
- `[SEP]`: separates entity types from input text.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bidirectional Encoder&lt;/strong&gt;: A BiLM (e.g., DeBERTa-v3) encodes both entity types and the input text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span Representation Module&lt;/strong&gt;: Computes span embeddings from token representations using a feedforward network:
\[
S_{ij} = \text{FFN}(h_i \oplus h_j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity Representation Module&lt;/strong&gt;: Processes entity type embeddings via another FFN.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Matching Layer&lt;/strong&gt;: Calculates matching score:
\[
\phi(i, j, t) = \sigma(S_{ij}^T q_t)
\]
where $\sigma$ is the sigmoid function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Strategy&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Binary cross-entropy loss over span/type pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Source&lt;/strong&gt;: Trained on &lt;strong&gt;Pile-NER&lt;/strong&gt;, a dataset derived from The Pile corpus with 44.8k passages and 13k entity types. Labels were generated by &lt;strong&gt;ChatGPT&lt;/strong&gt;, acting as a &lt;em&gt;teacher model&lt;/em&gt; (data-level distillation).

    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;figure  id=&#34;figure-prompting-chatgpt-for-entity-extraction-in-pile-ner-dataset&#34;&gt;
      &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
        &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Prompting ChatGPT for entity extraction in Pile-NER dataset&#34; srcset=&#34;
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png 400w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_78fed6eb2a2853a0c989d1c62aa41340.png 760w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_1200x1200_fit_lanczos_3.png 1200w&#34;
                   src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png&#34;
                   width=&#34;749&#34;
                   height=&#34;265&#34;
                   loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
      &lt;/div&gt;&lt;figcaption&gt;
          Prompting ChatGPT for entity extraction in Pile-NER dataset
        &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Techniques for robustness&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;Negative sampling of entity types.&lt;/li&gt;
&lt;li&gt;Random shuffling and dropping of entity prompts.&lt;/li&gt;
&lt;li&gt;Span length cap (max 12 tokens) for efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-experimental-results&#34;&gt;4. Experimental Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For zero-shot evaluation, on 20 diverse NER benchmarks and out-of-domain (OOD) tasks, &lt;code&gt;GLiNER-L (0.3B)&lt;/code&gt; outperforms: ChatGPT, InstructUIE (11B), UniNER (13B) and even GoLLIE (7B) in most cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For multilingual performance without multilingual training: &lt;code&gt;GLiNER-Multi (mDeBERTa)&lt;/code&gt; surpasses ChatGPT on 8 out of 11 languages (e.g., Spanish, German, Russian). which shows &lt;strong&gt;strong generalization&lt;/strong&gt;, even on unseen scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With supervised fine-tuning, after fine-tuning on labeled datasets, GLiNER competes closely with or surpasses InstructUIE, performs nearly as well as UniNER (larger LLaMA-based model). Pretraining on Pile-NER improves data efficiency, especially with small datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiency and Scalability&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GLiNER allows &lt;strong&gt;parallel inference&lt;/strong&gt; for multiple entity types.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Time&lt;/strong&gt;: ~5 hours on a single A100 GPU for GLiNER-L.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parameter Sizes&lt;/strong&gt;: 50M (S), 90M (M), 300M (L), compared to 7Bâ€“13B in baselines.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-ablation-insights&#34;&gt;5. Ablation Insights&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Negative sampling (50%)&lt;/td&gt;
&lt;td&gt;Best F1 balance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dropping entity types&lt;/td&gt;
&lt;td&gt;+1.4 F1 on OOD datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deBERTa-v3 backbone&lt;/td&gt;
&lt;td&gt;Outperforms RoBERTa, BERT, ALBERT, ELECTRA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Earlier NER approaches include rule-based systems, sequence labeling (e.g., BiLSTM-CRF), and span classification.&lt;/li&gt;
&lt;li&gt;LLM-based models (e.g., InstructUIE, UniNER) use instruction-tuning or generation.&lt;/li&gt;
&lt;li&gt;GLiNER offers a &lt;strong&gt;middle ground&lt;/strong&gt;: lightweight yet capable of open-type NER.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h3&gt;
&lt;p&gt;GLiNER is a &lt;strong&gt;generalist, scalable, and high-performing model&lt;/strong&gt; for Named Entity Recognition that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bridges the gap between classic NER and large LLM-based models.&lt;/li&gt;
&lt;li&gt;Achieves &lt;strong&gt;state-of-the-art zero-shot results&lt;/strong&gt; with minimal resources.&lt;/li&gt;
&lt;li&gt;Demonstrates &lt;strong&gt;robust multilingual and cross-domain generalization&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes it an excellent candidate for real-world NER applications in &lt;strong&gt;low-resource, high-efficiency environments&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ðŸ”— &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/urchade/GLiNER&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/urchade/GLiNER&lt;/a&gt;&lt;br&gt;
ðŸ“„ &lt;strong&gt;Paper&lt;/strong&gt;: Urchade Zaratiana et al., &lt;em&gt;GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer&lt;/em&gt;, arXiv:2311.08526&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
