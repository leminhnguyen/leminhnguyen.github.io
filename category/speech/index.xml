<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Speech | leminhnguyen&#39;s blog</title>
    <link>https://leminhnguyen.github.io/category/speech/</link>
      <atom:link href="https://leminhnguyen.github.io/category/speech/index.xml" rel="self" type="application/rss+xml" />
    <description>Speech</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬© {2025} leminhnguyen</copyright><lastBuildDate>Mon, 28 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Speech</title>
      <link>https://leminhnguyen.github.io/category/speech/</link>
    </image>
    
    <item>
      <title>Speaker Diarization: From Traditional Methods to the Modern Models</title>
      <link>https://leminhnguyen.github.io/post/speech-research/speaker-diarization/</link>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/speaker-diarization/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
Speaker Diarization, the task of answering &lt;code&gt;‚ÄúWho spoken when?‚Äù&lt;/code&gt; - is an crucial component in many speech processing systems. From meeting transcription to customer service call analysis, diarization allows to segment signal by speakers, making down-stream tasks like speech-to-text, emotion analysis, or intent identification much more effective. The figure 1 below shows the speaker diarization results from my developed model on a youtube audio.














&lt;figure  id=&#34;figure-fig-1-the-diarization-results-from-my-developed-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 1: The diarization results from my developed model&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_ab611148729afd5bb6b2b44ff298383d.png 400w,
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_a1eab91924d3a385687f1ed0b6749f9d.png 760w,
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_ab611148729afd5bb6b2b44ff298383d.png&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 1: The diarization results from my developed model
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 10px&#34;&gt;
In this blog, I‚Äôll introduce the core concepts of speaker diarization, discover both traditional and end-to-end methods, and highligth one of the latest innovations in the field: Sortformer model. Whether you‚Äôre just getting started or looking to catch up recent innovations, this blog aims give you a comprehensive overview. 
&lt;/div&gt;
&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional Methods&lt;/li&gt;
&lt;li&gt;End-to-End Models&lt;/li&gt;
&lt;li&gt;New Breakthroughs in Diarization&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-traditional-methods&#34;&gt;1. Traditional Methods&lt;/h3&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-fig-2-traditional-speaker-diarization-pipeline&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 2: Traditional Speaker Diarization Pipeline&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_9bd4912a2e36e703ab217f84e7f104a5.png 400w,
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_59370a29b144f19cf032b93530b94476.png 760w,
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_9bd4912a2e36e703ab217f84e7f104a5.png&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 2: Traditional Speaker Diarization Pipeline
    &lt;/figcaption&gt;&lt;/figure&gt;
Traditional diarization systems often rely on modular pipelines, combining speaker embeddings (like i-vectors) with clustering algorithms such as Agglomerative Hierarchical Clustering (AHC). While effective, these systems require careful tuning and may struggle with overlapping speech. Those consist of many independent submodules that are optimized individually, namely being:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -10px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speech Detection and Segmentation:&lt;/strong&gt;¬†This step detects which regions of the audio contain speech and which are silent or contain noise, then splits the speech into chunks. It usually uses energy-based thresholds, voice activity detectors (VAD), or neural classifiers to separate speech from non-speech regions. Accurate VAD is critical because missed speech or false positives directly affect downstream segmentation and labeling. One of the most popular VAD algorithms is WebRTC VAD, which uses a combination of energy and spectral features to detect speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech Embedding:&lt;/strong&gt;¬†A neural network pre-trained on speaker recognition is used to derive a high-level representation of the speech segments. Those embeddings are vector representations that summarize the voice characteristics (a.k.a voice print). Early systems used MFCC (Mel-frequency cepstral coefficients), but more modern pipelines use i-vectors or x-vectors, which are compact representations capturing speaker identity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speaker Clustering:&lt;/strong&gt;¬†After extracting segment embeddings, we need to cluster the speech embeddings with a clustering algorithm (for example K-Means or spectral clustering). The clustering produces our desired diarization results, which consists of identifying the number of unique speakers (derived from the number of unique clusters) and assigning a speaker label to each embedding (or speech segment).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id=&#34;2-end-to-end-method&#34;&gt;2. End To End Method&lt;/h3&gt;
&lt;p&gt;End-to-end (E2E) diarization models aim to integrate the entire diarization process into a single neural network architecture, reducing the need for modular tuning and improving generalization. It usually inclues core crchitecture features such as:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -20px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Joint Learning: E2E models are trained to jointly optimize speech segmentation, speaker embedding extraction, and speaker assignment within one framework.&lt;/li&gt;
&lt;li&gt;Neural Encoders: Use convolutional neural networks (CNNs), recurrent neural networks (RNNs), or transformers to extract rich time-series representations from audio inputs.&lt;/li&gt;
&lt;li&gt;Attention Mechanisms: Incorporate self-attention layers to capture long-range dependencies across audio sequences, which is especially useful in handling speaker changes and overlapping speech.&lt;/li&gt;
&lt;li&gt;Loss Functions: Design specialized loss functions (e.g., permutation-invariant training) that help the model learn speaker assignments without being confused by label permutations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;2.1 Pyannote Audio&lt;/strong&gt;














&lt;figure  id=&#34;figure-fig-3-pyannote-audio-framework&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 3: Pyannote Audio Framework&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_bfcefe4f6bd63bec1b1ca774d5672d01.jpg 400w,
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_debe6adaba4f91716017823d1d736632.jpg 760w,
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_bfcefe4f6bd63bec1b1ca774d5672d01.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 3: Pyannote Audio Framework
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.2 Multi-Scale Diarization Nemo&lt;/strong&gt;
&lt;br&gt;
Speaker diarization faces a trade-off between accurately capturing speaker traits (which needs long audio segments) and achieving fine temporal resolution (which requires short segments). Traditional single-scale methods balance these but still leave gaps in accuracy, especially for short speaker turns common in conversation. To address this, a multi-scale approach is proposed, where speaker features are extracted at multiple segment lengths and combined using a multi-scale diarization decoder (MSDD). MSDD dynamically assigns weights to each scale using a CNN-based mechanism, improving diarization accuracy by balancing temporal precision and speaker representation quality.














&lt;figure  id=&#34;figure-fig-4-multi-scale-diarization-from-nemo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 4: Multi-Scale Diarization from Nemo&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_bbaec566d3120964c9f68e69f9945a3a.png 400w,
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_9ff8823fe4969344afa59efc019f539c.png 760w,
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_bbaec566d3120964c9f68e69f9945a3a.png&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 4: Multi-Scale Diarization from Nemo
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-problem-does-sort-loss-solve&#34;&gt;What Problem Does Sort Loss Solve?&lt;/h3&gt;
&lt;p&gt;Speaker diarization models predict &lt;strong&gt;who&lt;/strong&gt; is speaking at &lt;strong&gt;each frame&lt;/strong&gt; of audio. But ‚Äî &lt;strong&gt;the model doesn&amp;rsquo;t know speaker identities&lt;/strong&gt;! It only uses generic speaker labels (e.g., Speaker-0, Speaker-1). Traditional training needs to match predicted speakers to ground-truth speakers, trying every possible permutation (PIL) ‚Äî very expensive when many speakers exist!&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px;&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;&lt;strong&gt;Sortformer&lt;/strong&gt; solves this by introducing &lt;strong&gt;Sort Loss&lt;/strong&gt;:&lt;/p&gt;
&lt;ul style=&#34;margin-top: -15px; margin-bottom: 0; padding-left: 30px;&#34;&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34; markdown=&#34;1&#34;&gt;Sort speakers &lt;code&gt; by their speaking start time &lt;/code&gt; (Arrival Time Order ‚Äî ATO)&lt;/li&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34;&gt;Always treat the first speaker as Speaker-0, second as Speaker-1, etc&lt;/li&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34;&gt;No need for heavy permutation matching!&lt;/li&gt;
&lt;/div&gt;
&lt;h3 id=&#34;-what-is-the-permutation-problem-in-speaker-diarization&#34;&gt;üåü What Is the Permutation Problem in Speaker Diarization?&lt;/h3&gt;
&lt;p&gt;Speaker diarization systems assign speaker labels to segments of audio. But unlike speaker identification, the identities are generic &lt;code&gt; Speaker-0 &lt;/code&gt;, &lt;code&gt; Speaker-1 &lt;/code&gt;, etc. That creates a permutation problem: the system might label Speaker-A as Speaker-0 in one instance and Speaker-1 in another. Traditionally, this is handled using Permutation Invariant Loss (PIL) or Permutation Invariant Training (PIT):&lt;/p&gt;
&lt;ul style=&#34;margin-top: -15px; margin-bottom: 10px; padding-left: 30px;&#34;&gt;
&lt;li&gt;PIL checks all possible mappings of predicted labels to ground-truth and picks the one with the lowest loss.
&lt;/li&gt;
&lt;li&gt;It becomes expensive as the number of speakers increases: time complexity is &lt;code&gt;O(N!)&lt;/code&gt; or at best &lt;code&gt;O(N¬≥)&lt;/code&gt; using the Hungarian algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That‚Äôs where Sortformer introduces a breakthrough idea. Why not just sort speakers by who spoke first and train the model to always follow this order? This is the foundation of Sort Loss.&lt;/p&gt;
&lt;h3 id=&#34;how-sortformer-training-works&#34;&gt;How Sortformer Training Works&lt;/h3&gt;
&lt;p&gt;The training steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input audio&lt;/strong&gt; ‚ûî Extract frame-wise features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sort the ground-truth speakers&lt;/strong&gt; by their &lt;strong&gt;start time&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model predicts&lt;/strong&gt; frame-level speaker activities independently (using Sigmoid).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate Sort Loss&lt;/strong&gt;: Match model outputs with sorted true labels using &lt;strong&gt;Binary Cross-Entropy&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backpropagate&lt;/strong&gt; and update model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;‚úÖ Speakers who speak earlier are consistently mapped to earlier speaker labels during training!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-sort-loss-formula&#34;&gt;üìú Sort Loss Formula&lt;/h3&gt;
&lt;p&gt;The Sort Loss formula is:
$$L_{\text{Sort}}(Y, P) = \frac{1}{K} \sum_{k=1}^{K} \text{BCE}(y_{\eta(k)}, q_k)$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y$ = ground-truth speaker activities.&lt;/li&gt;
&lt;li&gt;$P$ = predicted speaker probabilities.&lt;/li&gt;
&lt;li&gt;$\eta(k)$ = the sorted index by arrival time.&lt;/li&gt;
&lt;li&gt;$K$ = number of speakers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BCE&lt;/strong&gt; = Binary Cross-Entropy loss for each speaker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‚úÖ Each speaker is evaluated independently.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-why-binary-cross-entropy-bce-not-normal-cross-entropy&#34;&gt;ü§î Why Binary Cross-Entropy (BCE), Not Normal Cross-Entropy?&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Cross Entropy (CE)&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Binary Cross Entropy (BCE)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Use case&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single-label classification&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multi-label classification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Output Activation&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Softmax (probabilities sum to 1)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sigmoid (independent probabilities)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Can handle overlaps?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;‚ùå No&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;‚úÖ Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Example&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pick one animal (cat, dog, rabbit)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pick all fruits you like (apple, banana, grape)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In speaker diarization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multiple speakers can talk at once&lt;/strong&gt; ‚ûî multi-label ‚ûî &lt;strong&gt;Binary Cross Entropy is needed&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Each speaker is predicted &lt;strong&gt;independently&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-tiny-example-of-sort-loss-in-action&#34;&gt;üî• Tiny Example of Sort Loss in Action&lt;/h3&gt;
&lt;p&gt;Suppose we have 2 speakers and 3 frames:&lt;/p&gt;
&lt;p&gt;Ground-truth (after sorting):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frame&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk0&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Predicted outputs:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frame&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk0&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Binary Cross Entropy is applied &lt;strong&gt;separately for each speaker&lt;/strong&gt;, and averaged over speakers.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-quick-summary-softmax-vs-sigmoid&#34;&gt;üß† Quick Summary: Softmax vs Sigmoid&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Softmax&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Sigmoid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sum of outputs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Not necessarily&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mutual exclusivity&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Application&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single-label classification (only 1 class active)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multi-label classification (multiple active)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Used with&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cross Entropy Loss&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Binary Cross Entropy Loss&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;‚úÖ &lt;strong&gt;Softmax&lt;/strong&gt; is used with Cross Entropy.&lt;br&gt;
‚úÖ &lt;strong&gt;Sigmoid&lt;/strong&gt; is used with Binary Cross Entropy.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-conclusion&#34;&gt;üì¶ Conclusion&lt;/h3&gt;
&lt;p&gt;‚úÖ Sortformer introduces a faster, more elegant solution for speaker diarization by sorting speakers by arrival time and applying simple Binary Cross-Entropy.&lt;/p&gt;
&lt;p&gt;‚úÖ BCE and Sigmoid are natural choices when multiple speakers can overlap.&lt;/p&gt;
&lt;p&gt;‚úÖ No more expensive permutation matching is needed!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-final-words&#34;&gt;üèÅ Final Words&lt;/h3&gt;
&lt;p&gt;This approach is simpler, faster, and works better for multi-speaker real-world conversations.
Stay tuned for more tutorials where we dive into multispeaker ASR models and joint training with speaker supervision!&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Why Entropy Matters in Machine Learning?</title>
      <link>https://leminhnguyen.github.io/post/machine-learning/entropy/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/machine-learning/entropy/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px margin-bottom: 10px&#34;&gt;














&lt;figure  id=&#34;figure-low-vs-high-entropy&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Low vs High Entropy&#34; srcset=&#34;
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_6bcaeffca1501bd96ec2cbe2bdf01b35.png 400w,
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_a693682445ad969ba8be8a82e9a88e93.png 760w,
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_6bcaeffca1501bd96ec2cbe2bdf01b35.png&#34;
               width=&#34;700&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Low vs High Entropy
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt; is a powerful and fundamental concept that quietly drives some of the most effective algorithms in machine learning. From decision trees to deep neural networks, entropy plays a central role in helping models navigate uncertainty and make better predictions.&lt;/p&gt;
&lt;h3 id=&#34;what-is-entropy&#34;&gt;What Is Entropy?&lt;/h3&gt;
&lt;p&gt;Originally a concept from thermodynamics, entropy measures the level of &lt;strong&gt;disorder&lt;/strong&gt; or &lt;strong&gt;uncertainty&lt;/strong&gt; in a system. In machine learning, it&amp;rsquo;s used to quantify how much unpredictability exists in a set of outcomes.&lt;/p&gt;
&lt;p&gt;Take a coin flip, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you flip a fair coin and get &lt;code&gt;[heads, tails, tails, heads]&lt;/code&gt;, there&amp;rsquo;s high entropy ‚Äî the outcomes are unpredictable.&lt;/li&gt;
&lt;li&gt;But if you flip a weighted coin and get &lt;code&gt;[tails, tails, tails, tails]&lt;/code&gt;, entropy is low ‚Äî the system is more predictable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High entropy&lt;/strong&gt; = low information gain (we learn less from each new example).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low entropy&lt;/strong&gt; = high information gain (we learn more from each new example).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entropy-in-decision-trees&#34;&gt;Entropy in Decision Trees&lt;/h3&gt;
&lt;p&gt;Entropy is the secret sauce behind decision trees. When a decision tree decides where to split the data, it doesn&amp;rsquo;t just guess‚Äîit asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúWhich feature split gives me the most certainty about what‚Äôs going on?‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It measures the entropy of each feature. The split that &lt;strong&gt;reduces entropy the most&lt;/strong&gt; (i.e., gives the most information gain) gets picked. It‚Äôs like asking:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúWhich question brings me closer to the truth?‚Äù&lt;/p&gt;
&lt;/blockquote&gt;














&lt;figure  id=&#34;figure-decision-tree-example-source-andre-ye&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Decision Tree Example (source: Andre Ye)&#34; srcset=&#34;
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_d8e79941847cb4700b891eda18c2fdd0.png 400w,
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_a3f99da1f104f0db8ae4fb4e95b76141.png 760w,
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_d8e79941847cb4700b891eda18c2fdd0.png&#34;
               width=&#34;700&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Decision Tree Example (source: Andre Ye)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For instance, if you&amp;rsquo;re building a tree to classify colors into red or blue, and one feature creates two groups that are nearly all red and all blue‚Äîthat‚Äôs &lt;strong&gt;low entropy&lt;/strong&gt;, and that feature becomes a &lt;strong&gt;high-value decision&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is why trees often start with the most informative feature at the top: to guide the rest of the tree with clarity and purpose.&lt;/p&gt;
&lt;h3 id=&#34;cross-entropy-in-neural-networks&#34;&gt;Cross-Entropy in Neural Networks&lt;/h3&gt;
&lt;p&gt;In deep learning, entropy shows up again‚Äîthis time in disguise, as &lt;strong&gt;cross-entropy&lt;/strong&gt;, a favorite loss function of neural networks. Imagine you&amp;rsquo;re training a model to classify images of cats and dogs. Cross-entropy doesn&amp;rsquo;t just care &lt;em&gt;which&lt;/em&gt; label the model picked‚Äîit cares &lt;em&gt;how confident&lt;/em&gt; the model was.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your model says ‚ÄúI‚Äôm 99% sure this is a cat‚Äù and it‚Äôs correct: great.&lt;/li&gt;
&lt;li&gt;If it says ‚Äú50-50, could be cat or dog‚Äù ‚Äî not so great.&lt;/li&gt;
&lt;li&gt;If it‚Äôs confidently wrong ‚Äî disaster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-entropy punishes bad guesses and rewards confident, correct predictions. It pushes the model to not just be right, but to be sure of why it&amp;rsquo;s right. Cross-entropy measures how many bits are needed to encode the true labels using the predicted distribution. The lower the value, the better the model&amp;rsquo;s predictions match the truth.&lt;/p&gt;
&lt;p&gt;This works beautifully with &lt;strong&gt;Softmax&lt;/strong&gt; and &lt;strong&gt;Sigmoid&lt;/strong&gt; activations, helping reduce issues like the vanishing gradient problem and giving models a smoother learning curve. This approach is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More dynamic than accuracy/error-based metrics.&lt;/li&gt;
&lt;li&gt;Better at handling confidence and probability.&lt;/li&gt;
&lt;li&gt;Less sensitive to data order or noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-concept-kl-divergence&#34;&gt;Related Concept: KL Divergence&lt;/h3&gt;
&lt;p&gt;Another flavor of entropy is &lt;strong&gt;Kullback‚ÄìLeibler divergence (KL divergence)&lt;/strong&gt;. Think of it as a way to measure the &amp;ldquo;distance&amp;rdquo; between two probability worlds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One world is what &lt;em&gt;actually&lt;/em&gt; happens (distribution &lt;strong&gt;p&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;The other is what your model &lt;em&gt;thinks&lt;/em&gt; will happen (distribution &lt;strong&gt;q&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KL divergence tells you how far off your model is‚Äîand how much it needs to learn. It‚Äôs like a map for loss, guiding your model back toward reality.&lt;/p&gt;
&lt;p&gt;GANs (Generative Adversarial Networks) use this idea to help the generator produce images that look increasingly real. The better it gets at mimicking the real distribution, the smaller the divergence.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;alt text&#34; srcset=&#34;
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_fe99a68586fcefd1732d76fb2c078899.png 400w,
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_d0f636f329a3297908d6bc22f42ca50b.png 760w,
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_fe99a68586fcefd1732d76fb2c078899.png&#34;
               width=&#34;700&#34;
               height=&#34;239&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;why-entropy-matters&#34;&gt;Why Entropy Matters&lt;/h3&gt;
&lt;p&gt;Unlike rigid metrics like accuracy, entropy-based measures capture the uncertainty and depth of the problem space. They allow models to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn better under uncertainty.&lt;/li&gt;
&lt;li&gt;Make probabilistic predictions.&lt;/li&gt;
&lt;li&gt;Avoid problems like vanishing gradients (especially when used with softmax or sigmoid activations).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you&amp;rsquo;re building a decision tree, training a neural network, or experimenting with probabilistic models, &lt;strong&gt;entropy is the invisible force guiding better decisions&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;Entropy might seem abstract at first, but it captures a truth at the heart of machine learning: we are always trying to reduce uncertainty. By optimizing for entropy-based metrics like information gain, cross-entropy, or KL divergence, we empower our models to learn faster, perform better, and make smarter predictions.&lt;/p&gt;
&lt;p&gt;Entropy is not just a formula. It‚Äôs a mindset. A way of accepting that knowledge is never perfect, but it &lt;em&gt;can&lt;/em&gt; be improved. When we train models with entropy in mind, we embrace the chaos‚Äîand turn it into clarity.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://medium.com/data-science/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/data-science/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;üîó &lt;a href=&#34;https://huggingface.co/blog/hexgrad/g2p&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://huggingface.co/blog/hexgrad/g2p&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LoRA-Whisper: A Scalable and Efficient Solution for Multilingual ASR</title>
      <link>https://leminhnguyen.github.io/post/speech-research/lora-whisper/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/lora-whisper/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h2 id=&#34;1-background--motivation&#34;&gt;&lt;strong&gt;1. Background &amp;amp; Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Automatic Speech Recognition (ASR) has made significant strides in recent years, particularly with the rise of large-scale multilingual models like OpenAI&amp;rsquo;s Whisper, Google USM, and Meta&amp;rsquo;s MMS. These models unlock possibilities for building speech recognition systems that support dozens ‚Äî or even hundreds ‚Äî of languages.&lt;br&gt;
However, building such multilingual ASR systems remains challenging due to:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px&#34;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Language Interference&lt;/strong&gt;: When multiple languages are trained in a shared model, performance may degrade due to data imbalance, dialectal accents, and language similarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Catastrophic Forgetting&lt;/strong&gt;: Fine-tuning a model on new languages often causes the model to forget previously learned languages, severely impacting recognition performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h2 id=&#34;2-the-proposed-solution-lora-whisper&#34;&gt;&lt;strong&gt;2. The Proposed Solution: LoRA-Whisper&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To tackle these two key challenges, researchers from Shanghai Jiao Tong University and Tencent AI Lab introduce &lt;a href=&#34;https://arxiv.org/pdf/2406.06619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LoRA-Whisper&lt;/strong&gt;&lt;/a&gt;, a parameter-efficient and extensible multilingual ASR framework based on the Whisper model and &lt;strong&gt;Low-Rank Adaptation (LoRA)&lt;/strong&gt;.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_4a9bda5537472758efffb93aa1a9bd94.png 400w,
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_ee9b2c2b586140e39f7e0fc69ebab6c6.png 760w,
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_4a9bda5537472758efffb93aa1a9bd94.png&#34;
               width=&#34;760&#34;
               height=&#34;446&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h4 id=&#34;what-is-lora&#34;&gt;&lt;strong&gt;What is LoRA?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;LoRA&lt;/strong&gt; (Low-Rank Adaptation) is a lightweight fine-tuning technique that freezes the original model weights and injects small, trainable low-rank matrices into certain layers (e.g., attention and feed-forward layers). This allows models to be efficiently adapted to new tasks or domains with minimal parameter overhead.&lt;/p&gt;
&lt;h4 id=&#34;how-lora-whisper-works&#34;&gt;How LoRA-Whisper Works&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For each language, a &lt;strong&gt;language-specific LoRA module&lt;/strong&gt; is attached to the Whisper model.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Whisper model remains frozen&lt;/strong&gt;, serving as a shared backbone.&lt;/li&gt;
&lt;li&gt;When recognizing a language, only the corresponding LoRA module is activated during inference.&lt;/li&gt;
&lt;li&gt;This design prevents language interference and ensures knowledge preservation of all previously learned languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;adding-new-languages-language-expansion&#34;&gt;Adding New Languages (Language Expansion)&lt;/h4&gt;
&lt;p&gt;LoRA-Whisper offers two innovative methods for expanding the model with new languages without retraining the entire model:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: -15px&#34;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LoRA Warm Start&lt;/strong&gt;: The LoRA module for a new language is initialized using the LoRA module of the most similar existing language (based on Whisper‚Äôs language ID probabilities).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LoRA Mixture of Experts (MoE)&lt;/strong&gt;: The system dynamically selects and combines LoRA modules from multiple similar languages during training and inference to aid the new language‚Äôs learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
These methods significantly improve adaptation quality while avoiding catastrophic forgetting.
&lt;h2 id=&#34;3-experimental-results&#34;&gt;&lt;strong&gt;3. Experimental Results&lt;/strong&gt;&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_8b58205bdaada31faf668f6735a58c0e.png 400w,
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_e87846e488630ab835c83f67ee08ef77.png 760w,
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_8b58205bdaada31faf668f6735a58c0e.png&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Experiments were conducted using MLS and FLEURS datasets across 8 languages. Highlights include:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: 0px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multilingual ASR&lt;/strong&gt;: LoRA-Whisper outperformed multilingual fine-tuning and came close to monolingual fine-tuning, using only ~5% of the trainable parameters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language Expansion&lt;/strong&gt;: Full fine-tuning with new languages caused up to 3√ó performance drop on existing languages. LoRA-Whisper maintained performance on existing languages while significantly improving WER (Word Error Rate) on new languages. LoRA warm start and LoRA MoE achieved 23% and 5% relative gains respectively over LoRA without similarity-based strategies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h4 id=&#34;ablation-study-does-language-similarity-help&#34;&gt;Ablation Study: Does Language Similarity Help?&lt;/h4&gt;
&lt;p&gt;Yes. The authors demonstrated that initializing a new language&amp;rsquo;s LoRA from a similar language&amp;rsquo;s LoRA consistently led to better performance. In contrast, initializing from an unrelated language could hurt performance ‚Äî even worse than training from scratch.&lt;/p&gt;
&lt;h4 id=&#34;limitations--future-work&#34;&gt;Limitations &amp;amp; Future Work&lt;/h4&gt;
&lt;p&gt;While LoRA-Whisper is scalable and efficient, one limitation is that model size increases linearly with the number of supported languages due to separate LoRA modules. Future directions include:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: 0px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Sharing LoRA modules among similar languages.&lt;/li&gt;
&lt;li&gt;Extending the approach to low-resource and code-switching scenarios.&lt;/li&gt;
&lt;li&gt;Integrating more advanced expert routing techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;4-conclusion&#34;&gt;&lt;strong&gt;4. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;LoRA-Whisper&lt;/strong&gt; offers a compelling solution for building scalable, customizable, and language-resilient ASR systems. By combining Whisper‚Äôs robust multilingual backbone with the adaptability of LoRA, the paper demonstrates an effective way to expand and maintain large ASR systems without catastrophic forgetting or performance trade-offs. This work marks a step forward in enabling more inclusive, efficient, and modular speech recognition systems ‚Äî critical for real-world multilingual applications.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Vietnamese Voice Conversion</title>
      <link>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/</link>
      <pubDate>Sat, 09 Mar 2024 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;div style=&#34;text-align:justify&#34;&gt;
This thesis develops a voice conversion model for Vietnamese based on the Phoneme Hallucinator model with 2 adoptions: (1) Add a Text2SSL module to get more context information before performing the KNN algorithm, (2) To create a more
diverse dataset we apply spectrogram-resize (SR) based data augmentation idea from Free-VC model which distorts speaker information without changing content information to generate more ‚Äùspeakers‚Äù.
&lt;/div&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-the-proposal-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The proposal model&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_402d308e4cc0466a6716761c67ac5260.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_5bbc6f5994e960a7413518bb425da12a.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_402d308e4cc0466a6716761c67ac5260.png&#34;
               width=&#34;760&#34;
               height=&#34;607&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The proposal model
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_54a6cd26a3842fb84b50dc5e8f9b4ac5.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_8f518ab3170d7857360ba1ffa07379d5.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_54a6cd26a3842fb84b50dc5e8f9b4ac5.png&#34;
               width=&#34;760&#34;
               height=&#34;351&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;div id=&#39;section-1&#39; class=&#39;section&#39; style=&#34;width: 100%&#34;&gt;
        &lt;h2&gt;Comparing different methods&lt;/h2&gt;
        &lt;p&gt;This section compares the baseline and the proposal model.&lt;/p&gt;
&lt;table style=&#34;width: 100%&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Source&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Target&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Baseline Model&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Proposal Model&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[trangntt]&lt;/b&gt; Female to Female Conversion &lt;td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[trangntt] &lt;/b&gt; Male to Female Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[nguyenlm] &lt;/b&gt; Male to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] phone_hallucinator_mn.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[nguyenlm] &lt;/b&gt; Female to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] phone_hallucinator.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[thanhpv] &lt;/b&gt; Male to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] phone_hallucinator_hr.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
      &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[thanhpv] &lt;/b&gt; Female to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;table&#34; style=&#34;width: 100%&#34;&gt;
&lt;/div&gt;
&lt;!-- ![Python](images/python.svg) --&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_ba247c54d0c340584a86934861cb6f46.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png&#34;
               width=&#34;720&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;</description>
    </item>
    
    <item>
      <title>Postnet Layer</title>
      <link>https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/</guid>
      <description>&lt;h2 id=&#34;-postnet-layer&#34;&gt;üí£ Postnet Layer&lt;/h2&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
In some end-to-end TTS models today, after the hidden representations are passed through the decoder we got the mel-spectrogram which contains the predictions of the acoustic features. Finally, the decoder predictions are passed over &lt;span style=&#39;font-weight:bold&#39;&gt; the Postnet layer which predicts residual information to improve the construction performance of the model &lt;/span&gt;. The section below notes some insights about &lt;span style=&#39;font-weight:bold&#39;&gt; the Postnet layer &lt;/span&gt; by me when learning TTS.
&lt;/div&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1908.11535.pdf&#34; style=&#34;text-align: justify; font-size: 20px;&#34;&gt; 
1. https://arxiv.org/pdf/1908.11535.pdf - 30 Aug 2019
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;In addition to the decoder, some systems have a post-net,
an additional network that predicts acoustic features. A post-net was originally introduced to convert acoustic features to different acoustic features that were suitable for an adopted waveform synthesis method, for example, from mel spectrograms to linear spectrograms [2] or mel spectrograms to vocoder parameters [4]. In recent studies the role of the post-net was to improve the acoustic features predicted by the decoder to improve quality further [5, 6]. The post-net introduces an additional loss term in the objective function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2008.03388.pdf&#34; style=&#34;text-align: justify; font-size: 20px;&#34;&gt; 2. https://arxiv.org/pdf/2008.03388.pdf - 11 Aug 2020&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Relative to DAR, C-DAR has three additional changes that
do not significantly impact naturalness or controllability, but
provide additional insights into F0 generation. First, a 5-layer
postnet [3] follows the autoregressive RNN. We find that this
postnet has the effect of reducing autoregressive sampling errors and tightening the posterior distribution around the argmax
(Figure 2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_3d454b8539ed4f3c7668c209efbf7722.png 400w,
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_59f0b4c02707bb74d1a3fa8996c51df0.png 760w,
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_3d454b8539ed4f3c7668c209efbf7722.png&#34;
               width=&#34;760&#34;
               height=&#34;432&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
  </channel>
</rss>
