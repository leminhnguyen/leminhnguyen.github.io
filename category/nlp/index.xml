<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | leminhnguyen&#39;s blog</title>
    <link>https://leminhnguyen.github.io/category/nlp/</link>
      <atom:link href="https://leminhnguyen.github.io/category/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© {2025} leminhnguyen</copyright><lastBuildDate>Sun, 07 Sep 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>NLP</title>
      <link>https://leminhnguyen.github.io/category/nlp/</link>
    </image>
    
    <item>
      <title>Why Do Language Models Hallucinate?</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/</link>
      <pubDate>Sun, 07 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Recently, OpenAI has just released the paper &lt;em&gt;&amp;ldquo;Why Language Models Hallucinate&amp;rdquo;&lt;/em&gt; by Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang (2025).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Abstraction:&lt;/strong&gt; Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such
“hallucinations” persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious—they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded—language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This “epidemic” of penalizing uncertain responses can only be addressed through a socio-technical
mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The paper takes a deep dive into one of the most persistent problems in large language models (LLMs): &lt;strong&gt;hallucinations&lt;/strong&gt;, where models generate confident but false statements. Unlike many studies that only describe symptoms or propose patchwork fixes, this work goes further to explain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why hallucinations naturally arise&lt;/strong&gt; during pretraining, even with perfectly clean data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Why they persist&lt;/strong&gt; after alignment and fine-tuning, largely due to how benchmarks reward “guessing” over honest uncertainty.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How we can mitigate them&lt;/strong&gt; by reforming evaluation methods, introducing confidence thresholds, and encouraging models to output &lt;em&gt;“I don’t know”&lt;/em&gt; when appropriate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, the authors argue that hallucinations are not mysterious flaws, but predictable statistical errors reinforced by current evaluation practices — and that solving them requires changing &lt;strong&gt;how we test and reward AI models&lt;/strong&gt;, not just tweaking training pipelines.&lt;/p&gt;
&lt;h2 id=&#34;1-what-is-hallucination&#34;&gt;1. What is hallucination?&lt;/h2&gt;
&lt;p&gt;Hallucinations are plausible but false statements generated by language models. They can show up in surprising ways, even for seemingly straightforward questions. For example, when we asked a widely used chatbot for the title of the PhD dissertation by Adam Tauman Kalai (an author of the mentioned paper), it confidently produced three different answers—none of them correct. When we asked for his birthday, it gave three different dates, likewise all wrong.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_b534ed861ec46db82fe3045598049fa8.png 400w,
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_fa278adf30fcc1cb058598869146f37b.png 760w,
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_b534ed861ec46db82fe3045598049fa8.png&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-findings&#34;&gt;2. Findings&lt;/h2&gt;
&lt;p&gt;Think about it like a multiple-choice test. If you do not know the answer but take a wild guess, you might get lucky and be right. Leaving it blank guarantees a zero. In the same way, when models are graded only on accuracy, the percentage of questions they get exactly right, they are encouraged to guess rather than say “I don’t know.”&lt;/p&gt;
&lt;p&gt;As another example, suppose a language model is asked for someone’s birthday but doesn’t know. If it guesses “September 10,” it has a 1-in-365 chance of being right. Saying “I don’t know” guarantees zero points. Over thousands of test questions, the guessing model ends up looking better on scoreboards than a careful model that admits uncertainty.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_98995976e1767bfa81a8d5381386826a.png 400w,
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_757c11071686734cf61f74173a51c61d.png 760w,
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_98995976e1767bfa81a8d5381386826a.png&#34;
               width=&#34;760&#34;
               height=&#34;283&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h3 id=&#34;21-pretraining-inevitably-introduces-errors&#34;&gt;2.1. Pretraining inevitably introduces errors&lt;/h3&gt;
&lt;p&gt;Pretraining is essentially a &lt;strong&gt;density estimation problem&lt;/strong&gt;: the model tries to approximate the probability distribution of language. The authors show that this is closely related to a &lt;strong&gt;binary classification problem&lt;/strong&gt;: deciding whether a given output is valid or invalid. From statistical learning theory, &lt;strong&gt;classification always has a non-zero error rate&lt;/strong&gt; → meaning LLMs cannot avoid mistakes. Even with &lt;strong&gt;perfect, error-free data&lt;/strong&gt;, hallucinations emerge because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some facts are &lt;strong&gt;rare or unique&lt;/strong&gt; (e.g., a birthday mentioned only once).&lt;/li&gt;
&lt;li&gt;When no pattern exists in data, the model faces &lt;strong&gt;epistemic uncertainty&lt;/strong&gt; (knowledge that is simply missing).&lt;/li&gt;
&lt;li&gt;This explains why LLMs do fine on common facts (like “Einstein’s birthday”) but hallucinate rare ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key concept:&lt;/strong&gt; &lt;em&gt;Singleton rate&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The fraction of facts that appear only once in training data.&lt;/li&gt;
&lt;li&gt;The higher the singleton rate, the higher the expected hallucination rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;22-post-training-reinforces-hallucinations&#34;&gt;2.2. Post-training reinforces hallucinations&lt;/h3&gt;
&lt;p&gt;After pretraining, models are fine-tuned (RLHF, RLAIF, DPO, etc.) to align with human preferences. Intuitively, one might expect this to reduce hallucinations. However, hallucinations persist because of how &lt;strong&gt;evaluation benchmarks are designed&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most benchmarks use &lt;strong&gt;binary grading&lt;/strong&gt; (correct = 1, wrong = 0).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“I don’t know”&lt;/em&gt; (IDK) or abstentions are treated as &lt;strong&gt;wrong (0 points)&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This setup &lt;strong&gt;rewards guessing&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;A model that always guesses when unsure scores higher than a model that truthfully admits uncertainty.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Analogy:
&lt;ul&gt;
&lt;li&gt;Like students on multiple-choice exams — guessing improves test scores, even if it produces confident wrong answers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As a result, LLMs are trained and evaluated in a permanent &lt;strong&gt;“test-taking mode”&lt;/strong&gt;, where bluffing is optimal.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;23-why-hallucinations-are-not-mysterious&#34;&gt;2.3. Why hallucinations are not mysterious&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hallucinations are &lt;strong&gt;not unique AI quirks&lt;/strong&gt;, but simply &lt;strong&gt;statistical classification errors under uncertainty&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;They persist because current benchmarks &lt;strong&gt;misalign incentives&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;A hallucination can improve benchmark performance.&lt;/li&gt;
&lt;li&gt;An honest abstention reduces performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thus, even advanced post-training cannot solve hallucinations if the evaluation system keeps rewarding them.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-proposed-solution&#34;&gt;3. Proposed Solution&lt;/h2&gt;
&lt;h3 id=&#34;redesign-evaluations-to-reward-honesty&#34;&gt;Redesign evaluations to reward honesty&lt;/h3&gt;
&lt;p&gt;The main recommendation: &lt;strong&gt;adjust scoring in existing benchmarks&lt;/strong&gt; rather than invent new hallucination-specific tests. Benchmarks should stop penalizing abstentions and instead &lt;strong&gt;give credit to uncertainty&lt;/strong&gt; when appropriate.&lt;/p&gt;
&lt;h3 id=&#34;explicit-confidence-targets&#34;&gt;Explicit confidence targets&lt;/h3&gt;
&lt;p&gt;Inspired by real-world exams (e.g., SAT, GRE, Indian JEE), introduce &lt;strong&gt;penalties for incorrect guesses&lt;/strong&gt; and &lt;strong&gt;neutral credit for IDK&lt;/strong&gt;. Example instruction added to each benchmark task: “Answer only if you are &amp;gt;75% confident.  &lt;code&gt;Correct answer: +1 point&lt;/code&gt;, &lt;code&gt;Wrong guess: –2 points&lt;/code&gt;, &lt;code&gt;IDK: 0 points&lt;/code&gt;. This ensures models learn when it’s &lt;strong&gt;better to abstain&lt;/strong&gt; rather than guess.&lt;/p&gt;
&lt;h3 id=&#34;behavioral-calibration&#34;&gt;Behavioral Calibration&lt;/h3&gt;
&lt;p&gt;Instead of only reporting probabilities, models should &lt;strong&gt;act in accordance with their confidence level&lt;/strong&gt;. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the model is &amp;lt;50% confident, it should output IDK.&lt;/li&gt;
&lt;li&gt;If highly confident, it should answer directly.&lt;/li&gt;
&lt;li&gt;This approach helps align model behavior with &lt;strong&gt;trustworthy communication&lt;/strong&gt;, reducing overconfident hallucinations.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-bonus&#34;&gt;4. Bonus&lt;/h2&gt;
&lt;p&gt;OpenAI claimed that ChatGPT also hallucinates. GPT‑5 has significantly fewer hallucinations especially when reasoning⁠, but they still occur. In terms of accuracy, the older OpenAI o4-mini model performs slightly better. However, its error rate (i.e., rate of hallucination) is significantly higher. Strategically guessing when uncertain improves accuracy but increases errors and hallucinations.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_b41c75f73f0984e4236251db5d8d6796.png 400w,
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_891a630ebc216a5d8504156e95e5a165.png 760w,
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_b41c75f73f0984e4236251db5d8d6796.png&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Why Entropy Matters in Machine Learning?</title>
      <link>https://leminhnguyen.github.io/post/machine-learning/entropy/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/machine-learning/entropy/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px margin-bottom: 10px&#34;&gt;














&lt;figure  id=&#34;figure-low-vs-high-entropy&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Low vs High Entropy&#34; srcset=&#34;
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_6bcaeffca1501bd96ec2cbe2bdf01b35.png 400w,
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_a693682445ad969ba8be8a82e9a88e93.png 760w,
               /post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/entropy_in_data_hu774da9d84015d9efdfc2798c918ad7df_132158_6bcaeffca1501bd96ec2cbe2bdf01b35.png&#34;
               width=&#34;700&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Low vs High Entropy
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt; is a powerful and fundamental concept that quietly drives some of the most effective algorithms in machine learning. From decision trees to deep neural networks, entropy plays a central role in helping models navigate uncertainty and make better predictions.&lt;/p&gt;
&lt;h3 id=&#34;what-is-entropy&#34;&gt;What Is Entropy?&lt;/h3&gt;
&lt;p&gt;Originally a concept from thermodynamics, entropy measures the level of &lt;strong&gt;disorder&lt;/strong&gt; or &lt;strong&gt;uncertainty&lt;/strong&gt; in a system. In machine learning, it&amp;rsquo;s used to quantify how much unpredictability exists in a set of outcomes.&lt;/p&gt;
&lt;p&gt;Take a coin flip, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you flip a fair coin and get &lt;code&gt;[heads, tails, tails, heads]&lt;/code&gt;, there&amp;rsquo;s high entropy — the outcomes are unpredictable.&lt;/li&gt;
&lt;li&gt;But if you flip a weighted coin and get &lt;code&gt;[tails, tails, tails, tails]&lt;/code&gt;, entropy is low — the system is more predictable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High entropy&lt;/strong&gt; = low information gain (we learn less from each new example).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low entropy&lt;/strong&gt; = high information gain (we learn more from each new example).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entropy-in-decision-trees&#34;&gt;Entropy in Decision Trees&lt;/h3&gt;
&lt;p&gt;Entropy is the secret sauce behind decision trees. When a decision tree decides where to split the data, it doesn&amp;rsquo;t just guess—it asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Which feature split gives me the most certainty about what’s going on?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It measures the entropy of each feature. The split that &lt;strong&gt;reduces entropy the most&lt;/strong&gt; (i.e., gives the most information gain) gets picked. It’s like asking:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Which question brings me closer to the truth?”&lt;/p&gt;
&lt;/blockquote&gt;














&lt;figure  id=&#34;figure-decision-tree-example-source-andre-ye&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Decision Tree Example (source: Andre Ye)&#34; srcset=&#34;
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_d8e79941847cb4700b891eda18c2fdd0.png 400w,
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_a3f99da1f104f0db8ae4fb4e95b76141.png 760w,
               /post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/decision_tree_hu2fc7734e24d263dd4c849451e1112706_65155_d8e79941847cb4700b891eda18c2fdd0.png&#34;
               width=&#34;700&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Decision Tree Example (source: Andre Ye)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For instance, if you&amp;rsquo;re building a tree to classify colors into red or blue, and one feature creates two groups that are nearly all red and all blue—that’s &lt;strong&gt;low entropy&lt;/strong&gt;, and that feature becomes a &lt;strong&gt;high-value decision&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is why trees often start with the most informative feature at the top: to guide the rest of the tree with clarity and purpose.&lt;/p&gt;
&lt;h3 id=&#34;cross-entropy-in-neural-networks&#34;&gt;Cross-Entropy in Neural Networks&lt;/h3&gt;
&lt;p&gt;In deep learning, entropy shows up again—this time in disguise, as &lt;strong&gt;cross-entropy&lt;/strong&gt;, a favorite loss function of neural networks. Imagine you&amp;rsquo;re training a model to classify images of cats and dogs. Cross-entropy doesn&amp;rsquo;t just care &lt;em&gt;which&lt;/em&gt; label the model picked—it cares &lt;em&gt;how confident&lt;/em&gt; the model was.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your model says “I’m 99% sure this is a cat” and it’s correct: great.&lt;/li&gt;
&lt;li&gt;If it says “50-50, could be cat or dog” — not so great.&lt;/li&gt;
&lt;li&gt;If it’s confidently wrong — disaster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-entropy punishes bad guesses and rewards confident, correct predictions. It pushes the model to not just be right, but to be sure of why it&amp;rsquo;s right. Cross-entropy measures how many bits are needed to encode the true labels using the predicted distribution. The lower the value, the better the model&amp;rsquo;s predictions match the truth.&lt;/p&gt;
&lt;p&gt;This works beautifully with &lt;strong&gt;Softmax&lt;/strong&gt; and &lt;strong&gt;Sigmoid&lt;/strong&gt; activations, helping reduce issues like the vanishing gradient problem and giving models a smoother learning curve. This approach is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More dynamic than accuracy/error-based metrics.&lt;/li&gt;
&lt;li&gt;Better at handling confidence and probability.&lt;/li&gt;
&lt;li&gt;Less sensitive to data order or noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-concept-kl-divergence&#34;&gt;Related Concept: KL Divergence&lt;/h3&gt;
&lt;p&gt;Another flavor of entropy is &lt;strong&gt;Kullback–Leibler divergence (KL divergence)&lt;/strong&gt;. Think of it as a way to measure the &amp;ldquo;distance&amp;rdquo; between two probability worlds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One world is what &lt;em&gt;actually&lt;/em&gt; happens (distribution &lt;strong&gt;p&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;The other is what your model &lt;em&gt;thinks&lt;/em&gt; will happen (distribution &lt;strong&gt;q&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KL divergence tells you how far off your model is—and how much it needs to learn. It’s like a map for loss, guiding your model back toward reality.&lt;/p&gt;
&lt;p&gt;GANs (Generative Adversarial Networks) use this idea to help the generator produce images that look increasingly real. The better it gets at mimicking the real distribution, the smaller the divergence.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;alt text&#34; srcset=&#34;
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_fe99a68586fcefd1732d76fb2c078899.png 400w,
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_d0f636f329a3297908d6bc22f42ca50b.png 760w,
               /post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/machine-learning/entropy/KL_huf221c4c03880c66097e85ad3bf7df6f9_52023_fe99a68586fcefd1732d76fb2c078899.png&#34;
               width=&#34;700&#34;
               height=&#34;239&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;why-entropy-matters&#34;&gt;Why Entropy Matters&lt;/h3&gt;
&lt;p&gt;Unlike rigid metrics like accuracy, entropy-based measures capture the uncertainty and depth of the problem space. They allow models to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn better under uncertainty.&lt;/li&gt;
&lt;li&gt;Make probabilistic predictions.&lt;/li&gt;
&lt;li&gt;Avoid problems like vanishing gradients (especially when used with softmax or sigmoid activations).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you&amp;rsquo;re building a decision tree, training a neural network, or experimenting with probabilistic models, &lt;strong&gt;entropy is the invisible force guiding better decisions&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;Entropy might seem abstract at first, but it captures a truth at the heart of machine learning: we are always trying to reduce uncertainty. By optimizing for entropy-based metrics like information gain, cross-entropy, or KL divergence, we empower our models to learn faster, perform better, and make smarter predictions.&lt;/p&gt;
&lt;p&gt;Entropy is not just a formula. It’s a mindset. A way of accepting that knowledge is never perfect, but it &lt;em&gt;can&lt;/em&gt; be improved. When we train models with entropy in mind, we embrace the chaos—and turn it into clarity.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;🔗 &lt;a href=&#34;https://medium.com/data-science/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/data-science/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🔗 &lt;a href=&#34;https://huggingface.co/blog/hexgrad/g2p&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://huggingface.co/blog/hexgrad/g2p&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding FlashAttention: Inner vs Outer Loop Optimization</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/flash-attention/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/flash-attention/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h3 id=&#34;understanding-flashattention-inner-vs-outer-loop-optimization&#34;&gt;Understanding FlashAttention: Inner vs Outer Loop Optimization&lt;/h3&gt;
&lt;p&gt;FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently.&lt;/p&gt;














&lt;figure  id=&#34;figure-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&#34; srcset=&#34;
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg 400w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_9aeed5689bdd5c41159c955e6ca65d96.jpg 760w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;297&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-what-problem-does-it-solve&#34;&gt;🚀 What Problem Does It Solve?&lt;/h3&gt;
&lt;p&gt;In traditional attention mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attention matrices like Q, K, and V are huge.&lt;/li&gt;
&lt;li&gt;GPU cores (CUDA cores) must fetch data from &lt;strong&gt;HBM (High Bandwidth Memory)&lt;/strong&gt; repeatedly.&lt;/li&gt;
&lt;li&gt;Each access to HBM is slow and inefficient.&lt;/li&gt;
&lt;li&gt;Shared memory (SRAM) exists but is &lt;strong&gt;not optimally used&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This leads to frequent memory transfers, under-utilized cores, and slow inference time.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-flashattention-to-the-rescue&#34;&gt;⚡ FlashAttention to the Rescue&lt;/h3&gt;
&lt;p&gt;FlashAttention solves this by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dividing Q, K, V matrices into smaller blocks&lt;/strong&gt; (e.g., 32x32).&lt;/li&gt;
&lt;li&gt;Copying each block from &lt;strong&gt;HBM to SRAM once&lt;/strong&gt; (not repeatedly).&lt;/li&gt;
&lt;li&gt;Performing &lt;strong&gt;all computations inside SRAM&lt;/strong&gt;, near the GPU cores.&lt;/li&gt;
&lt;li&gt;Writing results back to HBM only once per block.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dramatically reduces memory access overhead and accelerates attention computations.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-inner-loop-vs-outer-loop&#34;&gt;🔁 Inner Loop vs Outer Loop&lt;/h3&gt;
&lt;h4 id=&#34;outer-loop&#34;&gt;Outer Loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Responsible for &lt;strong&gt;loading blocks of K/V&lt;/strong&gt; from HBM to SRAM.&lt;/li&gt;
&lt;li&gt;Each iteration handles a large memory transfer.&lt;/li&gt;
&lt;li&gt;Runs &lt;strong&gt;infrequently&lt;/strong&gt; but handles heavy data movement.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inner-loop&#34;&gt;Inner Loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Executes &lt;strong&gt;on the data already in SRAM&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Performs matrix multiplications (Q×Kᵀ), softmax, and QK×V.&lt;/li&gt;
&lt;li&gt;Runs &lt;strong&gt;frequently&lt;/strong&gt; but operates on fast-access memory.&lt;/li&gt;
&lt;li&gt;Fast and efficient — no further HBM access needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-kitchen-example&#34;&gt;🧠 Analogy: Kitchen Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HBM&lt;/strong&gt; = Warehouse far away.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SRAM&lt;/strong&gt; = Workbench in your kitchen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outer loop&lt;/strong&gt; = You bring a tray of ingredients from warehouse to your kitchen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inner loop&lt;/strong&gt; = You cook the full meal using what&amp;rsquo;s already on your workbench.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional attention = you run back to the warehouse for every spoon of spice 😅&lt;br&gt;
FlashAttention = bring the whole spice rack once, cook in peace! 👨‍🍳&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-summary&#34;&gt;✅ Summary&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Traditional Attention&lt;/th&gt;
&lt;th&gt;FlashAttention&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Memory Access&lt;/td&gt;
&lt;td&gt;Frequent HBM access&lt;/td&gt;
&lt;td&gt;One-time block transfer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SRAM Usage&lt;/td&gt;
&lt;td&gt;Under-utilized&lt;/td&gt;
&lt;td&gt;Fully utilized per block&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Computation Location&lt;/td&gt;
&lt;td&gt;Mix of HBM and registers&lt;/td&gt;
&lt;td&gt;All in SRAM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speed&lt;/td&gt;
&lt;td&gt;Slower, memory bottleneck&lt;/td&gt;
&lt;td&gt;Much faster, memory-efficient&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;FlashAttention is a key breakthrough for making large models faster and more scalable — especially during inference.&lt;/p&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Attacks on Large Language Models (LLMs)</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</link>
      <pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Large Language Models (LLMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Adversarial attacks on large language models (LLMs) involve manipulating inputs to deceive the model into generating harmful, biased, or incorrect outputs. These attacks exploit the vulnerabilities of LLMs, which rely on patterns in training data to generate responses. Below is an overview of key concepts, types of attacks, implications, and defense strategies.&lt;/p&gt;














&lt;figure  id=&#34;figure-an-overview-of-threats-to-llm-based-applications-source-lillog-blog&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An overview of threats to LLM-based applications (source: Lil&amp;#39;Log Blog)&#34; srcset=&#34;
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png 400w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_39113d4dab022780f9de83ba4af61166.png 760w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png&#34;
               width=&#34;760&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An overview of threats to LLM-based applications (source: Lil&amp;rsquo;Log Blog)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-types-of-adversarial-attacks-on-llms&#34;&gt;&lt;strong&gt;1. Types of Adversarial Attacks on LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-evasion-attacks&#34;&gt;a. Evasion Attacks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Attackers modify input text (e.g., by altering words, punctuation, or structure) to trick the model into producing unintended outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding a few words like &amp;ldquo;I am a helpful assistant&amp;rdquo; to a prompt to manipulate the model&amp;rsquo;s response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Can lead to misinformation, phishing, or generation of harmful content.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-poisoning-attacks&#34;&gt;&lt;strong&gt;b. Poisoning Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Corrupting training data to influence the model&amp;rsquo;s behavior. Attackers inject malicious examples during training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Including biased or harmful data in the training set to make the model generate toxic responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Long-term degradation of model reliability and trustworthiness.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-injection-attacks&#34;&gt;&lt;strong&gt;c. Injection Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inserting malicious code or prompts into the input to alter the model&amp;rsquo;s execution flow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Using adversarial prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; to exploit the model&amp;rsquo;s tendency to follow instructions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Enables exploitation of model capabilities for malicious purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-data-poisoning&#34;&gt;&lt;strong&gt;d. Data Poisoning&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Similar to poisoning attacks but focuses on corrupting the training dataset to bias the model&amp;rsquo;s outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding fake user interactions that encourage the model to generate harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Systemic bias and ethical risks in model behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-model-inversion-attacks&#34;&gt;&lt;strong&gt;e. Model Inversion Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inferring sensitive information about the model&amp;rsquo;s training data by analyzing outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Reverse-engineering the model to reveal private data or patterns in the training set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Privacy breaches and exposure of proprietary information.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-implications-of-adversarial-attacks&#34;&gt;&lt;strong&gt;2. Implications of Adversarial Attacks&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security Risks&lt;/strong&gt;: Phishing, misinformation, and malware generation via manipulated prompts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ethical Concerns&lt;/strong&gt;: Reinforcement of biases, hate speech, or harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trust Erosion&lt;/strong&gt;: Users may lose confidence in LLMs for critical tasks like healthcare, finance, or legal advice.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operational Disruption&lt;/strong&gt;: Attackers could disrupt services by causing models to fail or produce incorrect outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-defense-mechanisms&#34;&gt;&lt;strong&gt;3. Defense Mechanisms&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-adversarial-training&#34;&gt;&lt;strong&gt;a. Adversarial Training&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models on adversarial examples to improve robustness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Introduce perturbed inputs during training to make the model resistant to attacks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires access to adversarial examples, which may be difficult to generate for LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-input-sanitization&#34;&gt;&lt;strong&gt;b. Input Sanitization&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Detect and filter malicious patterns in inputs (e.g., using regex or keyword matching).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Blocking suspicious prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; or &amp;ldquo;I am a helpful assistant.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May fail against sophisticated, subtle attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-model-ensembles&#34;&gt;&lt;strong&gt;c. Model Ensembles&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Use multiple models to cross-validate outputs and detect inconsistencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If one model generates a harmful response, others may flag it as anomalous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Increases computational overhead and complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-uncertainty-estimation&#34;&gt;&lt;strong&gt;d. Uncertainty Estimation&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models to estimate confidence in their outputs, flagging uncertain responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If the model is unsure about a prompt, it may refuse to generate a response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires careful calibration and may reduce usability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-prompt-engineering-defenses&#34;&gt;&lt;strong&gt;e. Prompt Engineering Defenses&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Design prompts to resist adversarial manipulation (e.g., using multi-step reasoning or safety checks).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Incorporating safety constraints like &amp;ldquo;Avoid harmful content&amp;rdquo; into the prompt.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May not fully prevent attacks, especially if the adversary tailors prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-research-and-tools&#34;&gt;&lt;strong&gt;4. Research and Tools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Papers&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Adversarial Examples for Neural Network Language Models&amp;rdquo;&lt;/strong&gt; (Emti et al.) – Explores adversarial examples in NLP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Prompt Injection Attacks on Language Models&amp;rdquo;&lt;/strong&gt; (Zhang et al.) – Demonstrates how prompts can be weaponized.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Defending Against Prompt Injection Attacks&amp;rdquo;&lt;/strong&gt; (Li et al.) – Proposes defenses against adversarial prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adversarial Text Generation Tools&lt;/strong&gt;: Generate adversarial examples for testing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Auditing Frameworks&lt;/strong&gt;: Analyze model behavior for biases or vulnerabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-challenges-and-future-directions&#34;&gt;&lt;strong&gt;5. Challenges and Future Directions&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Nature of Attacks&lt;/strong&gt;: Adversaries continuously evolve techniques, requiring ongoing research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balancing Safety and Usability&lt;/strong&gt;: Defenses must avoid overly restrictive measures that hinder model functionality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Domain Collaboration&lt;/strong&gt;: Combining insights from cybersecurity, NLP, and ethics to address risks holistically.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Adversarial attacks on LLMs pose significant risks to security, ethics, and trust. While defenses like adversarial training and input sanitization offer partial protection, the dynamic nature of these threats demands continuous innovation. Researchers and practitioners must prioritize robustness, transparency, and ethical considerations to ensure the safe deployment of LLMs in real-world applications.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GLiNER: A Generalist Model for Named Entity Recognition using Bidirectional Transformers</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/gliner/</link>
      <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/gliner/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h3 id=&#34;1-what-is-named-entity-recognition-ner&#34;&gt;1. What is Named Entity Recognition (NER)?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; is a fundamental task in Natural Language Processing (NLP) that involves &lt;strong&gt;identifying and classifying spans of text&lt;/strong&gt; that refer to real-world entities such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Persons&lt;/strong&gt; (e.g., &amp;ldquo;Albert Einstein&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organizations&lt;/strong&gt; (e.g., &amp;ldquo;United Nations&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Locations&lt;/strong&gt; (e.g., &amp;ldquo;Paris&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dates, Products, Diseases&lt;/strong&gt;, and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional NER systems are trained on a &lt;strong&gt;fixed set of entity types&lt;/strong&gt;, which limits their adaptability to new domains or tasks. Recently, &lt;strong&gt;Open NER&lt;/strong&gt; has emerged as a flexible paradigm that allows recognizing arbitrary entity types based on natural language instructions — a direction GLiNER directly embraces and enhances.&lt;/p&gt;
&lt;h3 id=&#34;2-overview&#34;&gt;2. Overview&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GLiNER&lt;/strong&gt; is a compact and general-purpose model for &lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; that leverages &lt;strong&gt;Bidirectional Transformers&lt;/strong&gt; (like BERT or DeBERTa) to extract arbitrary types of entities from text — without being constrained to a fixed label set. Unlike traditional NER models or large language models (LLMs) like ChatGPT, GLiNER is &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;efficient&lt;/strong&gt;, and designed for &lt;strong&gt;zero-shot generalization&lt;/strong&gt; across domains and languages.&lt;/p&gt;
&lt;p&gt;Traditional NER systems are limited by a fixed ontology of entity types. While LLMs (e.g., GPT-3, ChatGPT) allow open-type NER via prompting, they are &lt;strong&gt;computationally expensive&lt;/strong&gt;, &lt;strong&gt;slow (token-by-token decoding)&lt;/strong&gt;, and often impractical in production due to API cost and latency. GLiNER aims to:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: -15px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Retain the &lt;strong&gt;flexibility of LLMs&lt;/strong&gt; in handling arbitrary entity types.&lt;/li&gt;
&lt;li&gt;Achieve &lt;strong&gt;high performance&lt;/strong&gt; with &lt;strong&gt;orders of magnitude fewer parameters&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Enable &lt;strong&gt;parallel extraction&lt;/strong&gt; of entities rather than autoregressive generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id=&#34;3-model-architecture&#34;&gt;3. Model Architecture&lt;/h3&gt;
&lt;p&gt;GLiNER reframes NER as a &lt;strong&gt;semantic matching problem&lt;/strong&gt; between entity types and text spans in a &lt;strong&gt;shared latent space&lt;/strong&gt;.














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png 400w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_b31f86d47083376b16f5847cefd3ac4d.png 760w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Input Format&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ENT] person [ENT] organization [ENT] location [SEP] Text...
- `[ENT]`: special token preceding each entity type.
- `[SEP]`: separates entity types from input text.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bidirectional Encoder&lt;/strong&gt;: A BiLM (e.g., DeBERTa-v3) encodes both entity types and the input text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span Representation Module&lt;/strong&gt;: Computes span embeddings from token representations using a feedforward network:
\[
S_{ij} = \text{FFN}(h_i \oplus h_j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity Representation Module&lt;/strong&gt;: Processes entity type embeddings via another FFN.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Matching Layer&lt;/strong&gt;: Calculates matching score:
\[
\phi(i, j, t) = \sigma(S_{ij}^T q_t)
\]
where $\sigma$ is the sigmoid function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Strategy&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Binary cross-entropy loss over span/type pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Source&lt;/strong&gt;: Trained on &lt;strong&gt;Pile-NER&lt;/strong&gt;, a dataset derived from The Pile corpus with 44.8k passages and 13k entity types. Labels were generated by &lt;strong&gt;ChatGPT&lt;/strong&gt;, acting as a &lt;em&gt;teacher model&lt;/em&gt; (data-level distillation).

    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;figure  id=&#34;figure-prompting-chatgpt-for-entity-extraction-in-pile-ner-dataset&#34;&gt;
      &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
        &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Prompting ChatGPT for entity extraction in Pile-NER dataset&#34; srcset=&#34;
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png 400w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_78fed6eb2a2853a0c989d1c62aa41340.png 760w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_1200x1200_fit_lanczos_3.png 1200w&#34;
                   src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png&#34;
                   width=&#34;749&#34;
                   height=&#34;265&#34;
                   loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
      &lt;/div&gt;&lt;figcaption&gt;
          Prompting ChatGPT for entity extraction in Pile-NER dataset
        &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Techniques for robustness&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;Negative sampling of entity types.&lt;/li&gt;
&lt;li&gt;Random shuffling and dropping of entity prompts.&lt;/li&gt;
&lt;li&gt;Span length cap (max 12 tokens) for efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-experimental-results&#34;&gt;4. Experimental Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For zero-shot evaluation, on 20 diverse NER benchmarks and out-of-domain (OOD) tasks, &lt;code&gt;GLiNER-L (0.3B)&lt;/code&gt; outperforms: ChatGPT, InstructUIE (11B), UniNER (13B) and even GoLLIE (7B) in most cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For multilingual performance without multilingual training: &lt;code&gt;GLiNER-Multi (mDeBERTa)&lt;/code&gt; surpasses ChatGPT on 8 out of 11 languages (e.g., Spanish, German, Russian). which shows &lt;strong&gt;strong generalization&lt;/strong&gt;, even on unseen scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With supervised fine-tuning, after fine-tuning on labeled datasets, GLiNER competes closely with or surpasses InstructUIE, performs nearly as well as UniNER (larger LLaMA-based model). Pretraining on Pile-NER improves data efficiency, especially with small datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiency and Scalability&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GLiNER allows &lt;strong&gt;parallel inference&lt;/strong&gt; for multiple entity types.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Time&lt;/strong&gt;: ~5 hours on a single A100 GPU for GLiNER-L.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parameter Sizes&lt;/strong&gt;: 50M (S), 90M (M), 300M (L), compared to 7B–13B in baselines.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-ablation-insights&#34;&gt;5. Ablation Insights&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Negative sampling (50%)&lt;/td&gt;
&lt;td&gt;Best F1 balance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dropping entity types&lt;/td&gt;
&lt;td&gt;+1.4 F1 on OOD datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deBERTa-v3 backbone&lt;/td&gt;
&lt;td&gt;Outperforms RoBERTa, BERT, ALBERT, ELECTRA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Earlier NER approaches include rule-based systems, sequence labeling (e.g., BiLSTM-CRF), and span classification.&lt;/li&gt;
&lt;li&gt;LLM-based models (e.g., InstructUIE, UniNER) use instruction-tuning or generation.&lt;/li&gt;
&lt;li&gt;GLiNER offers a &lt;strong&gt;middle ground&lt;/strong&gt;: lightweight yet capable of open-type NER.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h3&gt;
&lt;p&gt;GLiNER is a &lt;strong&gt;generalist, scalable, and high-performing model&lt;/strong&gt; for Named Entity Recognition that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bridges the gap between classic NER and large LLM-based models.&lt;/li&gt;
&lt;li&gt;Achieves &lt;strong&gt;state-of-the-art zero-shot results&lt;/strong&gt; with minimal resources.&lt;/li&gt;
&lt;li&gt;Demonstrates &lt;strong&gt;robust multilingual and cross-domain generalization&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes it an excellent candidate for real-world NER applications in &lt;strong&gt;low-resource, high-efficiency environments&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;🔗 &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/urchade/GLiNER&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/urchade/GLiNER&lt;/a&gt;&lt;br&gt;
📄 &lt;strong&gt;Paper&lt;/strong&gt;: Urchade Zaratiana et al., &lt;em&gt;GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer&lt;/em&gt;, arXiv:2311.08526&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Comparing batch vs layer normalization</title>
      <link>https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/</guid>
      <description>&lt;h2 id=&#34;-batch-vs-layer-normalization&#34;&gt;💣 Batch vs Layer Normalization&lt;/h2&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
The purpose of normalization is to provide &lt;span style=&#39;font-weight:bold&#39;&gt; an uniform scale for the input data &lt;/span&gt; to avoid varing in huge range. The normalization method ensures there is no loss of information and even the range of values isn&#39;t affected. In spite of normalizing the input data, &lt;span style=&#39;font-weight:bold&#39;&gt; the value of activations of certain neurons in the hidden layers can start varying across a wide scale during the training process. &lt;/span&gt; This means the input to the neurons to the next hidden layer will also range across the wide range, bringing instability.














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_26e8274f2bf3d60894ddf4d1cdbfe1ca.webp 400w,
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_caee8622773b04039cc2c664010e6c40.webp 760w,
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_1200x1200_fit_q75_h2_lanczos_2.webp 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_26e8274f2bf3d60894ddf4d1cdbfe1ca.webp&#34;
               width=&#34;760&#34;
               height=&#34;444&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Batch Normalization Layer is applied for neural networks where the training is done in mini-batches. We divide the data into batches with a certain batch size and then pass it through the network. Batch normalization is applied on the neuron activation for all the samples in the mini-batch such that the mean of output lies close to 0 and the standard deviation lies close to 1. It also introduces two learning parameters gama and beta in its calculation which are all optimized during training. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Layer Normalization which addresses the drawbacks of batch normalization. This technique is not dependent on batches and the normalization is applied on the neuron for a single instance across all features. Here also mean activation remains close to 0 and mean standard deviation remains close to 1. &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span style=&#39;font-weight:bold; font-size: 18px;&#39;&gt; The key difference &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch Normalization depends on mini-batch size and may not work properly for smaller batch sizes. On the other hand, Layer normalization does not depend on mini-batch size.&lt;/li&gt;
&lt;li&gt;In batch normalization, input values of the same neuron for all the data in the mini-batch are normalized. Whereas in layer normalization, input values for all neurons in the same layer are normalized for each data sample.&lt;/li&gt;
&lt;li&gt;Batch normalization works better with fully connected layers and convolutional neural network (CNN) but it shows poor results with recurrent neural network (RNN). On the other hand, the main advantage of Layer normalization is that it works really well with RNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningknowledge.ai/keras-normalization-layers-explained-for-beginners-batch-normalization-vs-layer-normalization/#:~:text=Batch%20Normalization%20vs%20Layer%20Normalization,-Before%20wrapping%20up&amp;text=In%20batch%20normalization%2C%20input%20values,normalized%20for%20each%20data%20sample.&#34; style=&#34;text-align: justify; font-size: 15px;&#34;&gt;Batch-vs-Layer-Normalization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
