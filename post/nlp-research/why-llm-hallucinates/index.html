<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Minh Nguyen Le" />

  
  
  
    
  
  <meta name="description" content="Overview Recently, OpenAI has just released the paper &ldquo;Why Language Models Hallucinate&rdquo; by Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang (2025).
Abstraction: Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty." />

  
  <link rel="alternate" hreflang="en-us" href="https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/" />

  
  
  
    <meta name="theme-color" content="#2962ff" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d65872b95e794dda92dc12a85e1e04fa.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="leminhnguyen&#39;s blog" />
  <meta property="og:url" content="https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/" />
  <meta property="og:title" content="Why Do Language Models Hallucinate? | leminhnguyen&#39;s blog" />
  <meta property="og:description" content="Overview Recently, OpenAI has just released the paper &ldquo;Why Language Models Hallucinate&rdquo; by Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang (2025).
Abstraction: Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty." /><meta property="og:image" content="https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/featured.png" />
    <meta property="twitter:image" content="https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2025-09-07T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2025-09-07T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/"
  },
  "headline": "Why Do Language Models Hallucinate?",
  
  "image": [
    "https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/featured.png"
  ],
  
  "datePublished": "2025-09-07T00:00:00Z",
  "dateModified": "2025-09-07T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Minh Nguyen Le"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "leminhnguyen's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Overview Recently, OpenAI has just released the paper \u0026ldquo;Why Language Models Hallucinate\u0026rdquo; by Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang (2025).\nAbstraction: Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty."
}
</script>

  

  

  

  





  <title>Why Do Language Models Hallucinate? | leminhnguyen&#39;s blog</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="90b415ce91b5d880d050628be1200dbc" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6619cc29fe1a874b6ea1a0359aab5cfb.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/about/"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/speech/"><span>Speech</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/nlp/"><span>NLP</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/python/"><span>Python</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/linux/"><span>Linux</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Why Do Language Models Hallucinate?</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 7, 2025
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/nlp/">NLP</a>, <a href="/category/large-language-models/">Large Language Models</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <div style="text-align: justify; font-size: 15px; margin-top: 20px">
<h2 id="overview">Overview</h2>
<p>Recently, OpenAI has just released the paper <em>&ldquo;Why Language Models Hallucinate&rdquo;</em> by Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang (2025).</p>
<blockquote>
<p><strong>Abstraction:</strong> Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such
“hallucinations” persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious—they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded—language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This “epidemic” of penalizing uncertain responses can only be addressed through a socio-technical
mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.</p>
</blockquote>
<p>The paper takes a deep dive into one of the most persistent problems in large language models (LLMs): <strong>hallucinations</strong>, where models generate confident but false statements. Unlike many studies that only describe symptoms or propose patchwork fixes, this work goes further to explain:</p>
<ul>
<li><strong>Why hallucinations naturally arise</strong> during pretraining, even with perfectly clean data.</li>
<li><strong>Why they persist</strong> after alignment and fine-tuning, largely due to how benchmarks reward “guessing” over honest uncertainty.</li>
<li><strong>How we can mitigate them</strong> by reforming evaluation methods, introducing confidence thresholds, and encouraging models to output <em>“I don’t know”</em> when appropriate.</li>
</ul>
<p>In short, the authors argue that hallucinations are not mysterious flaws, but predictable statistical errors reinforced by current evaluation practices — and that solving them requires changing <strong>how we test and reward AI models</strong>, not just tweaking training pipelines.</p>
<h2 id="1-what-is-hallucination">1. What is hallucination?</h2>
<p>Hallucinations are plausible but false statements generated by language models. They can show up in surprising ways, even for seemingly straightforward questions. For example, when we asked a widely used chatbot for the title of the PhD dissertation by Adam Tauman Kalai (an author of the mentioned paper), it confidently produced three different answers—none of them correct. When we asked for his birthday, it gave three different dates, likewise all wrong.</p>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="" srcset="
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_b534ed861ec46db82fe3045598049fa8.png 400w,
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_fa278adf30fcc1cb058598869146f37b.png 760w,
               /post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_1200x1200_fit_lanczos_3.png 1200w"
               src="/post/nlp-research/why-llm-hallucinates/featured_hu0eb9eabc467679d57cd4aff4d1bfa367_2798381_b534ed861ec46db82fe3045598049fa8.png"
               width="760"
               height="507"
               loading="lazy" data-zoomable /></div>
  </div></figure>
<hr>
<h2 id="2-findings">2. Findings</h2>
<p>Think about it like a multiple-choice test. If you do not know the answer but take a wild guess, you might get lucky and be right. Leaving it blank guarantees a zero. In the same way, when models are graded only on accuracy, the percentage of questions they get exactly right, they are encouraged to guess rather than say “I don’t know.”</p>
<p>As another example, suppose a language model is asked for someone’s birthday but doesn’t know. If it guesses “September 10,” it has a 1-in-365 chance of being right. Saying “I don’t know” guarantees zero points. Over thousands of test questions, the guessing model ends up looking better on scoreboards than a careful model that admits uncertainty.</p>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="" srcset="
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_98995976e1767bfa81a8d5381386826a.png 400w,
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_757c11071686734cf61f74173a51c61d.png 760w,
               /post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_1200x1200_fit_lanczos_3.png 1200w"
               src="/post/nlp-research/why-llm-hallucinates/valid_invalid_huc1f5495ef27f8d1bcbf026607e0e9a9f_204948_98995976e1767bfa81a8d5381386826a.png"
               width="760"
               height="283"
               loading="lazy" data-zoomable /></div>
  </div></figure>
<h3 id="21-pretraining-inevitably-introduces-errors">2.1. Pretraining inevitably introduces errors</h3>
<p>Pretraining is essentially a <strong>density estimation problem</strong>: the model tries to approximate the probability distribution of language. The authors show that this is closely related to a <strong>binary classification problem</strong>: deciding whether a given output is valid or invalid. From statistical learning theory, <strong>classification always has a non-zero error rate</strong> → meaning LLMs cannot avoid mistakes. Even with <strong>perfect, error-free data</strong>, hallucinations emerge because:</p>
<ul>
<li>Some facts are <strong>rare or unique</strong> (e.g., a birthday mentioned only once).</li>
<li>When no pattern exists in data, the model faces <strong>epistemic uncertainty</strong> (knowledge that is simply missing).</li>
<li>This explains why LLMs do fine on common facts (like “Einstein’s birthday”) but hallucinate rare ones.</li>
</ul>
<p><strong>Key concept:</strong> <em>Singleton rate</em></p>
<ul>
<li>The fraction of facts that appear only once in training data.</li>
<li>The higher the singleton rate, the higher the expected hallucination rate.</li>
</ul>
<hr>
<h3 id="22-post-training-reinforces-hallucinations">2.2. Post-training reinforces hallucinations</h3>
<p>After pretraining, models are fine-tuned (RLHF, RLAIF, DPO, etc.) to align with human preferences. Intuitively, one might expect this to reduce hallucinations. However, hallucinations persist because of how <strong>evaluation benchmarks are designed</strong>:</p>
<ul>
<li>Most benchmarks use <strong>binary grading</strong> (correct = 1, wrong = 0).</li>
<li><em>“I don’t know”</em> (IDK) or abstentions are treated as <strong>wrong (0 points)</strong>.</li>
<li>This setup <strong>rewards guessing</strong>:
<ul>
<li>A model that always guesses when unsure scores higher than a model that truthfully admits uncertainty.</li>
</ul>
</li>
<li>Analogy:
<ul>
<li>Like students on multiple-choice exams — guessing improves test scores, even if it produces confident wrong answers.</li>
</ul>
</li>
<li>As a result, LLMs are trained and evaluated in a permanent <strong>“test-taking mode”</strong>, where bluffing is optimal.</li>
</ul>
<hr>
<h3 id="23-why-hallucinations-are-not-mysterious">2.3. Why hallucinations are not mysterious</h3>
<ul>
<li>Hallucinations are <strong>not unique AI quirks</strong>, but simply <strong>statistical classification errors under uncertainty</strong>.</li>
<li>They persist because current benchmarks <strong>misalign incentives</strong>:
<ul>
<li>A hallucination can improve benchmark performance.</li>
<li>An honest abstention reduces performance.</li>
</ul>
</li>
<li>Thus, even advanced post-training cannot solve hallucinations if the evaluation system keeps rewarding them.</li>
</ul>
<hr>
<h2 id="3-proposed-solution">3. Proposed Solution</h2>
<h3 id="redesign-evaluations-to-reward-honesty">Redesign evaluations to reward honesty</h3>
<p>The main recommendation: <strong>adjust scoring in existing benchmarks</strong> rather than invent new hallucination-specific tests. Benchmarks should stop penalizing abstentions and instead <strong>give credit to uncertainty</strong> when appropriate.</p>
<h3 id="explicit-confidence-targets">Explicit confidence targets</h3>
<p>Inspired by real-world exams (e.g., SAT, GRE, Indian JEE), introduce <strong>penalties for incorrect guesses</strong> and <strong>neutral credit for IDK</strong>. Example instruction added to each benchmark task: “Answer only if you are &gt;75% confident.  <code>Correct answer: +1 point</code>, <code>Wrong guess: –2 points</code>, <code>IDK: 0 points</code>. This ensures models learn when it’s <strong>better to abstain</strong> rather than guess.</p>
<h3 id="behavioral-calibration">Behavioral Calibration</h3>
<p>Instead of only reporting probabilities, models should <strong>act in accordance with their confidence level</strong>. For example:</p>
<ul>
<li>If the model is &lt;50% confident, it should output IDK.</li>
<li>If highly confident, it should answer directly.</li>
<li>This approach helps align model behavior with <strong>trustworthy communication</strong>, reducing overconfident hallucinations.</li>
</ul>
<hr>
<h2 id="4-bonus">4. Bonus</h2>
<p>OpenAI claimed that ChatGPT also hallucinates. GPT‑5 has significantly fewer hallucinations especially when reasoning⁠, but they still occur. In terms of accuracy, the older OpenAI o4-mini model performs slightly better. However, its error rate (i.e., rate of hallucination) is significantly higher. Strategically guessing when uncertain improves accuracy but increases errors and hallucinations.</p>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="" srcset="
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_b41c75f73f0984e4236251db5d8d6796.png 400w,
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_891a630ebc216a5d8504156e95e5a165.png 760w,
               /post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_1200x1200_fit_lanczos_3.png 1200w"
               src="/post/nlp-research/why-llm-hallucinates/compare_gpt5_vs_gpt4_huf6586f449bf7da7d25f101f091dd9987_72745_b41c75f73f0984e4236251db5d8d6796.png"
               width="760"
               height="400"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</div>
    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">deep learning</a>
  
  <a class="badge badge-light" href="/tag/llm/">llm</a>
  
  <a class="badge badge-light" href="/tag/hallucination/">hallucination</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/&amp;text=Why%20Do%20Language%20Models%20Hallucinate?" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/&amp;t=Why%20Do%20Language%20Models%20Hallucinate?" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Why%20Do%20Language%20Models%20Hallucinate?&amp;body=https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/&amp;title=Why%20Do%20Language%20Models%20Hallucinate?" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Why%20Do%20Language%20Models%20Hallucinate?%20https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://leminhnguyen.github.io/post/nlp-research/why-llm-hallucinates/&amp;title=Why%20Do%20Language%20Models%20Hallucinate?" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://leminhnguyen.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/minh-nguyen-le/avatar_hue73c4865dd3562863accf819672b4d4e_72777_270x270_fill_lanczos_center_3.png" alt="Minh Nguyen Le"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://leminhnguyen.github.io/">Minh Nguyen Le</a></h5>
      <h6 class="card-subtitle">AI Engineer</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/about/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/minhnguyen.le.180072/" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://stackoverflow.com/users/10629841/leminhnguyen" target="_blank" rel="noopener">
        <i class="fab fa-stack-overflow"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/minh-nguyen-le-a83b0419b/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/leminhnguyen" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>












<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/speech-research/speaker-diarization/" rel="prev">Speaker Diarization: From Traditional Methods to the Modern Models</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/nlp-research/adversarial-attacks/">Adversarial Attacks on Large Language Models (LLMs)</a></li>
      
      <li><a href="/post/nlp-research/flash-attention/">Understanding FlashAttention: Inner vs Outer Loop Optimization</a></li>
      
      <li><a href="/post/nlp-research/gliner/">GLiNER: A Generalist Model for Named Entity Recognition using Bidirectional Transformers</a></li>
      
      <li><a href="/post/speech-research/speaker-diarization/">Speaker Diarization: From Traditional Methods to the Modern Models</a></li>
      
      <li><a href="/post/speech-research/lora-whisper/">LoRA-Whisper: A Scalable and Efficient Solution for Multilingual ASR</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    © {2025} leminhnguyen
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.5a3a4e7cbc7b4e121b2d29312cf8ad59.js"></script>

    
    
    
      
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.44f54723226744971bcdd9c0ca685057.js"></script>

    






</body>
</html>
