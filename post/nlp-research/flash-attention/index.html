<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Minh Nguyen Le" />

  
  
  
    
  
  <meta name="description" content="Understanding FlashAttention: Inner vs Outer Loop Optimization FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently." />

  
  <link rel="alternate" hreflang="en-us" href="https://leminhnguyen.github.io/post/nlp-research/flash-attention/" />

  
  
  
    <meta name="theme-color" content="#2962ff" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d65872b95e794dda92dc12a85e1e04fa.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://leminhnguyen.github.io/post/nlp-research/flash-attention/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="leminhnguyen&#39;s blog" />
  <meta property="og:url" content="https://leminhnguyen.github.io/post/nlp-research/flash-attention/" />
  <meta property="og:title" content="Understanding FlashAttention: Inner vs Outer Loop Optimization | leminhnguyen&#39;s blog" />
  <meta property="og:description" content="Understanding FlashAttention: Inner vs Outer Loop Optimization FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently." /><meta property="og:image" content="https://leminhnguyen.github.io/post/nlp-research/flash-attention/featured.webp" />
    <meta property="twitter:image" content="https://leminhnguyen.github.io/post/nlp-research/flash-attention/featured.webp" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2025-04-28T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2025-04-28T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://leminhnguyen.github.io/post/nlp-research/flash-attention/"
  },
  "headline": "Understanding FlashAttention: Inner vs Outer Loop Optimization",
  
  "image": [
    "https://leminhnguyen.github.io/post/nlp-research/flash-attention/featured.webp"
  ],
  
  "datePublished": "2025-04-28T00:00:00Z",
  "dateModified": "2025-04-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Minh Nguyen Le"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "leminhnguyen's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Understanding FlashAttention: Inner vs Outer Loop Optimization FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently."
}
</script>

  

  

  

  





  <title>Understanding FlashAttention: Inner vs Outer Loop Optimization | leminhnguyen&#39;s blog</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="1b77e2e3f028b434bbe70d5d6a47d7ae" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6619cc29fe1a874b6ea1a0359aab5cfb.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/about/"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/speech/"><span>Speech</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/nlp/"><span>NLP</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/python/"><span>Python</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/category/linux/"><span>Linux</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Understanding FlashAttention: Inner vs Outer Loop Optimization</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 28, 2025
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    2 min read
  </span>
  

  
  
  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/nlp/">NLP</a>, <a href="/category/flashattention/">FlashAttention</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <div style="text-align: justify; font-size: 15px; margin-top: 20px">
<h3 id="understanding-flashattention-inner-vs-outer-loop-optimization">Understanding FlashAttention: Inner vs Outer Loop Optimization</h3>
<p>FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently.</p>














<figure  id="figure-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" srcset="
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg 400w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_9aeed5689bdd5c41159c955e6ca65d96.jpg 760w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_1200x1200_fit_q75_lanczos.jpg 1200w"
               src="/post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg"
               width="760"
               height="297"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
    </figcaption></figure>
<hr>
<h3 id="-what-problem-does-it-solve">🚀 What Problem Does It Solve?</h3>
<p>In traditional attention mechanisms:</p>
<ul>
<li>Attention matrices like Q, K, and V are huge.</li>
<li>GPU cores (CUDA cores) must fetch data from <strong>HBM (High Bandwidth Memory)</strong> repeatedly.</li>
<li>Each access to HBM is slow and inefficient.</li>
<li>Shared memory (SRAM) exists but is <strong>not optimally used</strong>.</li>
</ul>
<p>This leads to frequent memory transfers, under-utilized cores, and slow inference time.</p>
<hr>
<h3 id="-flashattention-to-the-rescue">⚡ FlashAttention to the Rescue</h3>
<p>FlashAttention solves this by:</p>
<ul>
<li><strong>Dividing Q, K, V matrices into smaller blocks</strong> (e.g., 32x32).</li>
<li>Copying each block from <strong>HBM to SRAM once</strong> (not repeatedly).</li>
<li>Performing <strong>all computations inside SRAM</strong>, near the GPU cores.</li>
<li>Writing results back to HBM only once per block.</li>
</ul>
<p>This dramatically reduces memory access overhead and accelerates attention computations.</p>
<hr>
<h3 id="-inner-loop-vs-outer-loop">🔁 Inner Loop vs Outer Loop</h3>
<h4 id="outer-loop">Outer Loop</h4>
<ul>
<li>Responsible for <strong>loading blocks of K/V</strong> from HBM to SRAM.</li>
<li>Each iteration handles a large memory transfer.</li>
<li>Runs <strong>infrequently</strong> but handles heavy data movement.</li>
</ul>
<h4 id="inner-loop">Inner Loop</h4>
<ul>
<li>Executes <strong>on the data already in SRAM</strong>.</li>
<li>Performs matrix multiplications (Q×Kᵀ), softmax, and QK×V.</li>
<li>Runs <strong>frequently</strong> but operates on fast-access memory.</li>
<li>Fast and efficient — no further HBM access needed.</li>
</ul>
<hr>
<h3 id="-analogy-kitchen-example">🧠 Analogy: Kitchen Example</h3>
<ul>
<li><strong>HBM</strong> = Warehouse far away.</li>
<li><strong>SRAM</strong> = Workbench in your kitchen.</li>
<li><strong>Outer loop</strong> = You bring a tray of ingredients from warehouse to your kitchen.</li>
<li><strong>Inner loop</strong> = You cook the full meal using what&rsquo;s already on your workbench.</li>
</ul>
<p>Traditional attention = you run back to the warehouse for every spoon of spice 😅<br>
FlashAttention = bring the whole spice rack once, cook in peace! 👨‍🍳</p>
<hr>
<h3 id="-summary">✅ Summary</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Traditional Attention</th>
<th>FlashAttention</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Access</td>
<td>Frequent HBM access</td>
<td>One-time block transfer</td>
</tr>
<tr>
<td>SRAM Usage</td>
<td>Under-utilized</td>
<td>Fully utilized per block</td>
</tr>
<tr>
<td>Computation Location</td>
<td>Mix of HBM and registers</td>
<td>All in SRAM</td>
</tr>
<tr>
<td>Speed</td>
<td>Slower, memory bottleneck</td>
<td>Much faster, memory-efficient</td>
</tr>
</tbody>
</table>
<p>FlashAttention is a key breakthrough for making large models faster and more scalable — especially during inference.</p>
</figure>
    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">deep learning</a>
  
  <a class="badge badge-light" href="/tag/nlp/">nlp</a>
  
  <a class="badge badge-light" href="/tag/flash-attention/">flash attention</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://leminhnguyen.github.io/post/nlp-research/flash-attention/&amp;text=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://leminhnguyen.github.io/post/nlp-research/flash-attention/&amp;t=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization&amp;body=https://leminhnguyen.github.io/post/nlp-research/flash-attention/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://leminhnguyen.github.io/post/nlp-research/flash-attention/&amp;title=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization%20https://leminhnguyen.github.io/post/nlp-research/flash-attention/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://leminhnguyen.github.io/post/nlp-research/flash-attention/&amp;title=Understanding%20FlashAttention:%20Inner%20vs%20Outer%20Loop%20Optimization" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://leminhnguyen.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/minh-nguyen-le/avatar_hue73c4865dd3562863accf819672b4d4e_72777_270x270_fill_lanczos_center_3.png" alt="Minh Nguyen Le"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://leminhnguyen.github.io/">Minh Nguyen Le</a></h5>
      <h6 class="card-subtitle">AI Engineer</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/about/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.facebook.com/minhnguyen.le.180072/" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://stackoverflow.com/users/10629841/leminhnguyen" target="_blank" rel="noopener">
        <i class="fab fa-stack-overflow"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/minh-nguyen-le-a83b0419b/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/leminhnguyen" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>












<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/speech-research/speaker-diarization/" rel="next">Speaker Diarization: From Traditional Methods to the Modern Models</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/nlp-research/adversarial-attacks/" rel="prev">Adversarial Attacks on Large Language Models (LLMs)</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/nlp-research/adversarial-attacks/">Adversarial Attacks on Large Language Models (LLMs)</a></li>
      
      <li><a href="/post/speech-research/lora-whisper/">LoRA-Whisper: A Scalable and Efficient Solution for Multilingual ASR</a></li>
      
      <li><a href="/post/speech-research/speaker-diarization/">Speaker Diarization: From Traditional Methods to the Modern Models</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    © {2025} leminhnguyen
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.5a3a4e7cbc7b4e121b2d29312cf8ad59.js"></script>

    
    
    
      
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.44f54723226744971bcdd9c0ca685057.js"></script>

    






</body>
</html>
