<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>leminhnguyen&#39;s blog</title>
    <link>https://leminhnguyen.github.io/</link>
      <atom:link href="https://leminhnguyen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>leminhnguyen&#39;s blog</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© {2025} leminhnguyen</copyright><lastBuildDate>Mon, 28 Apr 2025 12:00:00 +0000</lastBuildDate>
    <image>
      <url>https://leminhnguyen.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>leminhnguyen&#39;s blog</title>
      <link>https://leminhnguyen.github.io/</link>
    </image>
    
    <item>
      <title>Handy Bash Snippets and Linux Tips</title>
      <link>https://leminhnguyen.github.io/post/linux-learns/helpful-commands/</link>
      <pubDate>Mon, 28 Apr 2025 12:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/linux-learns/helpful-commands/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;br&gt;
In my day-to-day work with Linux systems and development environments, I&#39;ve collected a variety of useful command-line snippets and troubleshooting notes.  This blog post shares some of my favorites from counting files to fixing display issues designed to boost your productivity and make your life easier. 
&lt;h3 id=&#34;1-quickly-count-files-in-a-folder&#34;&gt;1. Quickly Count Files in a Folder&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;fcount() { ls -1q &amp;quot;$1&amp;quot; | wc -l; }
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;br&gt;
&lt;code&gt;fcount /home/nguyenlm/folder&lt;/code&gt; → &lt;code&gt;27&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-force-fix-broken-cuda-installation&#34;&gt;2. Force Fix Broken CUDA Installation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get -o Dpkg::Options::=&amp;quot;--force-overwrite&amp;quot; install --fix-broken
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-count-non-blank-lines-in-a-file&#34;&gt;3. Count Non-Blank Lines in a File&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nbl-count() { grep -cve &#39;^\s*$&#39; &amp;quot;$1&amp;quot;; }
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;br&gt;
&lt;code&gt;nbl-count file.txt&lt;/code&gt; → &lt;code&gt;10&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-run-docker-without-sudo&#34;&gt;4. Run Docker Without Sudo&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo chmod 666 /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt;: This gives broad access to Docker socket. In production, add user to &lt;code&gt;docker&lt;/code&gt; group instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-fix-second-monitor-detection-in-ubuntu&#34;&gt;5. Fix Second Monitor Detection in Ubuntu&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get purge &#39;nvidia*&#39;
sudo add-apt-repository ppa:graphics-drivers
sudo apt-get update
sudo ubuntu-drivers autoinstall
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-fix-invalid-mit-magic-cookie-1-error-javafx-display-issue&#34;&gt;6. Fix &amp;ldquo;Invalid MIT-MAGIC-COOKIE-1&amp;rdquo; Error (JavaFX Display Issue)&lt;/h3&gt;
&lt;p&gt;First, check your active DISPLAY:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;who
# Example: user :1 2017-10-12 21:58 (:1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the correct environment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DISPLAY=:1.0
zenity --info --text &amp;quot;foobar&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip: Check &lt;code&gt;.bashrc&lt;/code&gt;, &lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;/etc/environment&lt;/code&gt;, or desktop environment configs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://bbs.archlinux.org/viewtopic.php?id=230828&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arch Linux Forum&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;7-check-folder-size-quickly&#34;&gt;7. Check Folder Size Quickly&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sizeof() {
    du -h --max-depth=0 &amp;quot;$1&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Example:&lt;br&gt;
&lt;code&gt;sizeof BOOK&lt;/code&gt; → &lt;code&gt;895M    BOOK/&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;8-sync-files-from-local-to-remote-using-rsync&#34;&gt;8. Sync Files from Local to Remote Using Rsync&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rsync -aPz -e &amp;quot;ssh -p port&amp;quot; local_folder/ user@remote_host:remote_folder
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip: Add &lt;code&gt;-n&lt;/code&gt; for dry-run to preview changes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://linuxize.com/post/how-to-use-rsync-for-local-and-remote-data-transfer-and-synchronization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rsync Command in Linux - Linuxize&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;9-find-and-kill-specific-processes&#34;&gt;9. Find and Kill Specific Processes&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kill $(ps aux | grep &#39;[p]rocess.py&#39; | awk &#39;{print $2}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;[p]&lt;/code&gt; prevents the grep command from appearing in the process list.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;10-recover-a-lost-tmux-session&#34;&gt;10. Recover a Lost Tmux Session&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pkill -USR1 tmux
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;These snippets have saved me countless hours when working with Linux environments, machine learning servers, and production systems.  Feel free to bookmark or adapt them to fit your own workflow! ⚡&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>LoRA-Whisper: A Scalable and Efficient Solution for Multilingual ASR</title>
      <link>https://leminhnguyen.github.io/post/speech-research/lora-whisper/</link>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/lora-whisper/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h2 id=&#34;1-background--motivation&#34;&gt;&lt;strong&gt;1. Background &amp;amp; Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Automatic Speech Recognition (ASR) has made significant strides in recent years, particularly with the rise of large-scale multilingual models like OpenAI&amp;rsquo;s Whisper, Google USM, and Meta&amp;rsquo;s MMS. These models unlock possibilities for building speech recognition systems that support dozens — or even hundreds — of languages.&lt;br&gt;
However, building such multilingual ASR systems remains challenging due to:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px&#34;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Language Interference&lt;/strong&gt;: When multiple languages are trained in a shared model, performance may degrade due to data imbalance, dialectal accents, and language similarities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Catastrophic Forgetting&lt;/strong&gt;: Fine-tuning a model on new languages often causes the model to forget previously learned languages, severely impacting recognition performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h2 id=&#34;2-the-proposed-solution-lora-whisper&#34;&gt;&lt;strong&gt;2. The Proposed Solution: LoRA-Whisper&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To tackle these two key challenges, researchers from Shanghai Jiao Tong University and Tencent AI Lab introduce &lt;a href=&#34;https://arxiv.org/pdf/2406.06619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LoRA-Whisper&lt;/strong&gt;&lt;/a&gt;, a parameter-efficient and extensible multilingual ASR framework based on the Whisper model and &lt;strong&gt;Low-Rank Adaptation (LoRA)&lt;/strong&gt;.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_4a9bda5537472758efffb93aa1a9bd94.png 400w,
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_ee9b2c2b586140e39f7e0fc69ebab6c6.png 760w,
               /post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/lora-whisper/LoRA%20warm%20start%20and%20MoE_hu6fbb7d968e144fe5cedba710fb29a1f0_142765_4a9bda5537472758efffb93aa1a9bd94.png&#34;
               width=&#34;760&#34;
               height=&#34;446&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h4 id=&#34;what-is-lora&#34;&gt;&lt;strong&gt;What is LoRA?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;LoRA&lt;/strong&gt; (Low-Rank Adaptation) is a lightweight fine-tuning technique that freezes the original model weights and injects small, trainable low-rank matrices into certain layers (e.g., attention and feed-forward layers). This allows models to be efficiently adapted to new tasks or domains with minimal parameter overhead.&lt;/p&gt;
&lt;h4 id=&#34;how-lora-whisper-works&#34;&gt;How LoRA-Whisper Works&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For each language, a &lt;strong&gt;language-specific LoRA module&lt;/strong&gt; is attached to the Whisper model.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Whisper model remains frozen&lt;/strong&gt;, serving as a shared backbone.&lt;/li&gt;
&lt;li&gt;When recognizing a language, only the corresponding LoRA module is activated during inference.&lt;/li&gt;
&lt;li&gt;This design prevents language interference and ensures knowledge preservation of all previously learned languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;adding-new-languages-language-expansion&#34;&gt;Adding New Languages (Language Expansion)&lt;/h4&gt;
&lt;p&gt;LoRA-Whisper offers two innovative methods for expanding the model with new languages without retraining the entire model:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: -15px&#34;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LoRA Warm Start&lt;/strong&gt;: The LoRA module for a new language is initialized using the LoRA module of the most similar existing language (based on Whisper’s language ID probabilities).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LoRA Mixture of Experts (MoE)&lt;/strong&gt;: The system dynamically selects and combines LoRA modules from multiple similar languages during training and inference to aid the new language’s learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
These methods significantly improve adaptation quality while avoiding catastrophic forgetting.
&lt;h2 id=&#34;3-experimental-results&#34;&gt;&lt;strong&gt;3. Experimental Results&lt;/strong&gt;&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_8b58205bdaada31faf668f6735a58c0e.png 400w,
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_e87846e488630ab835c83f67ee08ef77.png 760w,
               /post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/lora-whisper/lora_whisper_results_hu76a423228b00eeb3d77b99a44969647a_162684_8b58205bdaada31faf668f6735a58c0e.png&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Experiments were conducted using MLS and FLEURS datasets across 8 languages. Highlights include:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: 0px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multilingual ASR&lt;/strong&gt;: LoRA-Whisper outperformed multilingual fine-tuning and came close to monolingual fine-tuning, using only ~5% of the trainable parameters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language Expansion&lt;/strong&gt;: Full fine-tuning with new languages caused up to 3× performance drop on existing languages. LoRA-Whisper maintained performance on existing languages while significantly improving WER (Word Error Rate) on new languages. LoRA warm start and LoRA MoE achieved 23% and 5% relative gains respectively over LoRA without similarity-based strategies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h4 id=&#34;ablation-study-does-language-similarity-help&#34;&gt;Ablation Study: Does Language Similarity Help?&lt;/h4&gt;
&lt;p&gt;Yes. The authors demonstrated that initializing a new language&amp;rsquo;s LoRA from a similar language&amp;rsquo;s LoRA consistently led to better performance. In contrast, initializing from an unrelated language could hurt performance — even worse than training from scratch.&lt;/p&gt;
&lt;h4 id=&#34;limitations--future-work&#34;&gt;Limitations &amp;amp; Future Work&lt;/h4&gt;
&lt;p&gt;While LoRA-Whisper is scalable and efficient, one limitation is that model size increases linearly with the number of supported languages due to separate LoRA modules. Future directions include:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: 0px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Sharing LoRA modules among similar languages.&lt;/li&gt;
&lt;li&gt;Extending the approach to low-resource and code-switching scenarios.&lt;/li&gt;
&lt;li&gt;Integrating more advanced expert routing techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;4-conclusion&#34;&gt;&lt;strong&gt;4. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;LoRA-Whisper&lt;/strong&gt; offers a compelling solution for building scalable, customizable, and language-resilient ASR systems. By combining Whisper’s robust multilingual backbone with the adaptability of LoRA, the paper demonstrates an effective way to expand and maintain large ASR systems without catastrophic forgetting or performance trade-offs. This work marks a step forward in enabling more inclusive, efficient, and modular speech recognition systems — critical for real-world multilingual applications.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Speaker Diarization: From Traditional Methods to the Modern Models</title>
      <link>https://leminhnguyen.github.io/post/speech-research/speaker-diarization/</link>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/speaker-diarization/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
Speaker Diarization, the task of answering &lt;code&gt;“Who spoken when?”&lt;/code&gt; - is an crucial component in many speech processing systems. From meeting transcription to customer service call analysis, diarization allows to segment signal by speakers, making down-stream tasks like speech-to-text, emotion analysis, or intent identification much more effective. The figure 1 below shows the speaker diarization results from my developed model on a youtube audio.














&lt;figure  id=&#34;figure-fig-1-the-diarization-results-from-my-developed-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 1: The diarization results from my developed model&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_ab611148729afd5bb6b2b44ff298383d.png 400w,
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_a1eab91924d3a385687f1ed0b6749f9d.png 760w,
               /post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/speaker_diarization_voicelanding_hu28a540502b6e220f67c6fa946091d3da_64895_ab611148729afd5bb6b2b44ff298383d.png&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 1: The diarization results from my developed model
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 10px&#34;&gt;
In this blog, I’ll introduce the core concepts of speaker diarization, discover both traditional and end-to-end methods, and highligth one of the latest innovations in the field: Sortformer model. Whether you’re just getting started or looking to catch up recent innovations, this blog aims give you a comprehensive overview. 
&lt;/div&gt;
&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional Methods&lt;/li&gt;
&lt;li&gt;End-to-End Models&lt;/li&gt;
&lt;li&gt;New Breakthroughs in Diarization&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-traditional-methods&#34;&gt;1. Traditional Methods&lt;/h3&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-fig-2-traditional-speaker-diarization-pipeline&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 2: Traditional Speaker Diarization Pipeline&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_9bd4912a2e36e703ab217f84e7f104a5.png 400w,
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_59370a29b144f19cf032b93530b94476.png 760w,
               /post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/traditional_diar_hua7f002cd508fa95ba83464a48feb282c_171994_9bd4912a2e36e703ab217f84e7f104a5.png&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 2: Traditional Speaker Diarization Pipeline
    &lt;/figcaption&gt;&lt;/figure&gt;
Traditional diarization systems often rely on modular pipelines, combining speaker embeddings (like i-vectors) with clustering algorithms such as Agglomerative Hierarchical Clustering (AHC). While effective, these systems require careful tuning and may struggle with overlapping speech. Those consist of many independent submodules that are optimized individually, namely being:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -10px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speech Detection and Segmentation:&lt;/strong&gt; This step detects which regions of the audio contain speech and which are silent or contain noise, then splits the speech into chunks. It usually uses energy-based thresholds, voice activity detectors (VAD), or neural classifiers to separate speech from non-speech regions. Accurate VAD is critical because missed speech or false positives directly affect downstream segmentation and labeling. One of the most popular VAD algorithms is WebRTC VAD, which uses a combination of energy and spectral features to detect speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech Embedding:&lt;/strong&gt; A neural network pre-trained on speaker recognition is used to derive a high-level representation of the speech segments. Those embeddings are vector representations that summarize the voice characteristics (a.k.a voice print). Early systems used MFCC (Mel-frequency cepstral coefficients), but more modern pipelines use i-vectors or x-vectors, which are compact representations capturing speaker identity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speaker Clustering:&lt;/strong&gt; After extracting segment embeddings, we need to cluster the speech embeddings with a clustering algorithm (for example K-Means or spectral clustering). The clustering produces our desired diarization results, which consists of identifying the number of unique speakers (derived from the number of unique clusters) and assigning a speaker label to each embedding (or speech segment).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id=&#34;2-end-to-end-method&#34;&gt;2. End To End Method&lt;/h3&gt;
&lt;p&gt;End-to-end (E2E) diarization models aim to integrate the entire diarization process into a single neural network architecture, reducing the need for modular tuning and improving generalization. It usually inclues core crchitecture features such as:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -20px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Joint Learning: E2E models are trained to jointly optimize speech segmentation, speaker embedding extraction, and speaker assignment within one framework.&lt;/li&gt;
&lt;li&gt;Neural Encoders: Use convolutional neural networks (CNNs), recurrent neural networks (RNNs), or transformers to extract rich time-series representations from audio inputs.&lt;/li&gt;
&lt;li&gt;Attention Mechanisms: Incorporate self-attention layers to capture long-range dependencies across audio sequences, which is especially useful in handling speaker changes and overlapping speech.&lt;/li&gt;
&lt;li&gt;Loss Functions: Design specialized loss functions (e.g., permutation-invariant training) that help the model learn speaker assignments without being confused by label permutations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;2.1 Pyannote Audio&lt;/strong&gt;














&lt;figure  id=&#34;figure-fig-3-pyannote-audio-framework&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 3: Pyannote Audio Framework&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_bfcefe4f6bd63bec1b1ca774d5672d01.jpg 400w,
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_debe6adaba4f91716017823d1d736632.jpg 760w,
               /post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/pyannote_audio_pipeline_hu397e532eafa394da5f18400726a029a2_95311_bfcefe4f6bd63bec1b1ca774d5672d01.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 3: Pyannote Audio Framework
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.2 Multi-Scale Diarization Nemo&lt;/strong&gt;
&lt;br&gt;
Speaker diarization faces a trade-off between accurately capturing speaker traits (which needs long audio segments) and achieving fine temporal resolution (which requires short segments). Traditional single-scale methods balance these but still leave gaps in accuracy, especially for short speaker turns common in conversation. To address this, a multi-scale approach is proposed, where speaker features are extracted at multiple segment lengths and combined using a multi-scale diarization decoder (MSDD). MSDD dynamically assigns weights to each scale using a CNN-based mechanism, improving diarization accuracy by balancing temporal precision and speaker representation quality.














&lt;figure  id=&#34;figure-fig-4-multi-scale-diarization-from-nemo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig 4: Multi-Scale Diarization from Nemo&#34; srcset=&#34;
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_bbaec566d3120964c9f68e69f9945a3a.png 400w,
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_9ff8823fe4969344afa59efc019f539c.png 760w,
               /post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/speaker-diarization/nemo_unispeech_pipeline_hue6ea386f8be205e2031329979c69f704_676196_bbaec566d3120964c9f68e69f9945a3a.png&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 4: Multi-Scale Diarization from Nemo
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-problem-does-sort-loss-solve&#34;&gt;What Problem Does Sort Loss Solve?&lt;/h3&gt;
&lt;p&gt;Speaker diarization models predict &lt;strong&gt;who&lt;/strong&gt; is speaking at &lt;strong&gt;each frame&lt;/strong&gt; of audio. But — &lt;strong&gt;the model doesn&amp;rsquo;t know speaker identities&lt;/strong&gt;! It only uses generic speaker labels (e.g., Speaker-0, Speaker-1). Traditional training needs to match predicted speakers to ground-truth speakers, trying every possible permutation (PIL) — very expensive when many speakers exist!&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px;&#34; markdown=&#34;1&#34;&gt;
&lt;p&gt;&lt;strong&gt;Sortformer&lt;/strong&gt; solves this by introducing &lt;strong&gt;Sort Loss&lt;/strong&gt;:&lt;/p&gt;
&lt;ul style=&#34;margin-top: -15px; margin-bottom: 0; padding-left: 30px;&#34;&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34; markdown=&#34;1&#34;&gt;Sort speakers &lt;code&gt; by their speaking start time &lt;/code&gt; (Arrival Time Order — ATO)&lt;/li&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34;&gt;Always treat the first speaker as Speaker-0, second as Speaker-1, etc&lt;/li&gt;
&lt;li style=&#34;margin-bottom: 0px;&#34;&gt;No need for heavy permutation matching!&lt;/li&gt;
&lt;/div&gt;
&lt;h3 id=&#34;-what-is-the-permutation-problem-in-speaker-diarization&#34;&gt;🌟 What Is the Permutation Problem in Speaker Diarization?&lt;/h3&gt;
&lt;p&gt;Speaker diarization systems assign speaker labels to segments of audio. But unlike speaker identification, the identities are generic &lt;code&gt; Speaker-0 &lt;/code&gt;, &lt;code&gt; Speaker-1 &lt;/code&gt;, etc. That creates a permutation problem: the system might label Speaker-A as Speaker-0 in one instance and Speaker-1 in another. Traditionally, this is handled using Permutation Invariant Loss (PIL) or Permutation Invariant Training (PIT):&lt;/p&gt;
&lt;ul style=&#34;margin-top: -15px; margin-bottom: 10px; padding-left: 30px;&#34;&gt;
&lt;li&gt;PIL checks all possible mappings of predicted labels to ground-truth and picks the one with the lowest loss.
&lt;/li&gt;
&lt;li&gt;It becomes expensive as the number of speakers increases: time complexity is &lt;code&gt;O(N!)&lt;/code&gt; or at best &lt;code&gt;O(N³)&lt;/code&gt; using the Hungarian algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s where Sortformer introduces a breakthrough idea. Why not just sort speakers by who spoke first and train the model to always follow this order? This is the foundation of Sort Loss.&lt;/p&gt;
&lt;h3 id=&#34;how-sortformer-training-works&#34;&gt;How Sortformer Training Works&lt;/h3&gt;
&lt;p&gt;The training steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input audio&lt;/strong&gt; ➔ Extract frame-wise features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sort the ground-truth speakers&lt;/strong&gt; by their &lt;strong&gt;start time&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model predicts&lt;/strong&gt; frame-level speaker activities independently (using Sigmoid).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate Sort Loss&lt;/strong&gt;: Match model outputs with sorted true labels using &lt;strong&gt;Binary Cross-Entropy&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backpropagate&lt;/strong&gt; and update model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;✅ Speakers who speak earlier are consistently mapped to earlier speaker labels during training!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-sort-loss-formula&#34;&gt;📜 Sort Loss Formula&lt;/h3&gt;
&lt;p&gt;The Sort Loss formula is:
$$L_{\text{Sort}}(Y, P) = \frac{1}{K} \sum_{k=1}^{K} \text{BCE}(y_{\eta(k)}, q_k)$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y$ = ground-truth speaker activities.&lt;/li&gt;
&lt;li&gt;$P$ = predicted speaker probabilities.&lt;/li&gt;
&lt;li&gt;$\eta(k)$ = the sorted index by arrival time.&lt;/li&gt;
&lt;li&gt;$K$ = number of speakers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BCE&lt;/strong&gt; = Binary Cross-Entropy loss for each speaker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;✅ Each speaker is evaluated independently.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-why-binary-cross-entropy-bce-not-normal-cross-entropy&#34;&gt;🤔 Why Binary Cross-Entropy (BCE), Not Normal Cross-Entropy?&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Feature&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Cross Entropy (CE)&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Binary Cross Entropy (BCE)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Use case&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single-label classification&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multi-label classification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Output Activation&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Softmax (probabilities sum to 1)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sigmoid (independent probabilities)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Can handle overlaps?&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;❌ No&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;✅ Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Example&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pick one animal (cat, dog, rabbit)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pick all fruits you like (apple, banana, grape)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In speaker diarization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multiple speakers can talk at once&lt;/strong&gt; ➔ multi-label ➔ &lt;strong&gt;Binary Cross Entropy is needed&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Each speaker is predicted &lt;strong&gt;independently&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-tiny-example-of-sort-loss-in-action&#34;&gt;🔥 Tiny Example of Sort Loss in Action&lt;/h3&gt;
&lt;p&gt;Suppose we have 2 speakers and 3 frames:&lt;/p&gt;
&lt;p&gt;Ground-truth (after sorting):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frame&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk0&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Predicted outputs:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Frame&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk0&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;spk1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;t3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Binary Cross Entropy is applied &lt;strong&gt;separately for each speaker&lt;/strong&gt;, and averaged over speakers.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-quick-summary-softmax-vs-sigmoid&#34;&gt;🧠 Quick Summary: Softmax vs Sigmoid&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Softmax&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Sigmoid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Sum of outputs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Not necessarily&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Mutual exclusivity&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Yes&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Application&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single-label classification (only 1 class active)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Multi-label classification (multiple active)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Used with&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cross Entropy Loss&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Binary Cross Entropy Loss&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;✅ &lt;strong&gt;Softmax&lt;/strong&gt; is used with Cross Entropy.&lt;br&gt;
✅ &lt;strong&gt;Sigmoid&lt;/strong&gt; is used with Binary Cross Entropy.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-conclusion&#34;&gt;📦 Conclusion&lt;/h3&gt;
&lt;p&gt;✅ Sortformer introduces a faster, more elegant solution for speaker diarization by sorting speakers by arrival time and applying simple Binary Cross-Entropy.&lt;/p&gt;
&lt;p&gt;✅ BCE and Sigmoid are natural choices when multiple speakers can overlap.&lt;/p&gt;
&lt;p&gt;✅ No more expensive permutation matching is needed!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-final-words&#34;&gt;🏁 Final Words&lt;/h3&gt;
&lt;p&gt;This approach is simpler, faster, and works better for multi-speaker real-world conversations.
Stay tuned for more tutorials where we dive into multispeaker ASR models and joint training with speaker supervision!&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Understanding FlashAttention: Inner vs Outer Loop Optimization</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/flash-attention/</link>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/flash-attention/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h3 id=&#34;understanding-flashattention-inner-vs-outer-loop-optimization&#34;&gt;Understanding FlashAttention: Inner vs Outer Loop Optimization&lt;/h3&gt;
&lt;p&gt;FlashAttention is a groundbreaking optimization technique for computing attention in Transformer models. It drastically improves performance by reducing memory bottlenecks and utilizing GPU memory more efficiently.&lt;/p&gt;














&lt;figure  id=&#34;figure-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&#34; srcset=&#34;
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg 400w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_9aeed5689bdd5c41159c955e6ca65d96.jpg 760w,
               /post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/flash-attention/flashattn_banner_2_hubb76ad5ff4fdede90787cccc6e0e85a5_322273_c91538f784224d53b4c6b0409cebbc5d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;297&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-what-problem-does-it-solve&#34;&gt;🚀 What Problem Does It Solve?&lt;/h3&gt;
&lt;p&gt;In traditional attention mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attention matrices like Q, K, and V are huge.&lt;/li&gt;
&lt;li&gt;GPU cores (CUDA cores) must fetch data from &lt;strong&gt;HBM (High Bandwidth Memory)&lt;/strong&gt; repeatedly.&lt;/li&gt;
&lt;li&gt;Each access to HBM is slow and inefficient.&lt;/li&gt;
&lt;li&gt;Shared memory (SRAM) exists but is &lt;strong&gt;not optimally used&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This leads to frequent memory transfers, under-utilized cores, and slow inference time.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-flashattention-to-the-rescue&#34;&gt;⚡ FlashAttention to the Rescue&lt;/h3&gt;
&lt;p&gt;FlashAttention solves this by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dividing Q, K, V matrices into smaller blocks&lt;/strong&gt; (e.g., 32x32).&lt;/li&gt;
&lt;li&gt;Copying each block from &lt;strong&gt;HBM to SRAM once&lt;/strong&gt; (not repeatedly).&lt;/li&gt;
&lt;li&gt;Performing &lt;strong&gt;all computations inside SRAM&lt;/strong&gt;, near the GPU cores.&lt;/li&gt;
&lt;li&gt;Writing results back to HBM only once per block.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dramatically reduces memory access overhead and accelerates attention computations.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-inner-loop-vs-outer-loop&#34;&gt;🔁 Inner Loop vs Outer Loop&lt;/h3&gt;
&lt;h4 id=&#34;outer-loop&#34;&gt;Outer Loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Responsible for &lt;strong&gt;loading blocks of K/V&lt;/strong&gt; from HBM to SRAM.&lt;/li&gt;
&lt;li&gt;Each iteration handles a large memory transfer.&lt;/li&gt;
&lt;li&gt;Runs &lt;strong&gt;infrequently&lt;/strong&gt; but handles heavy data movement.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inner-loop&#34;&gt;Inner Loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Executes &lt;strong&gt;on the data already in SRAM&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Performs matrix multiplications (Q×Kᵀ), softmax, and QK×V.&lt;/li&gt;
&lt;li&gt;Runs &lt;strong&gt;frequently&lt;/strong&gt; but operates on fast-access memory.&lt;/li&gt;
&lt;li&gt;Fast and efficient — no further HBM access needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-kitchen-example&#34;&gt;🧠 Analogy: Kitchen Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HBM&lt;/strong&gt; = Warehouse far away.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SRAM&lt;/strong&gt; = Workbench in your kitchen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outer loop&lt;/strong&gt; = You bring a tray of ingredients from warehouse to your kitchen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inner loop&lt;/strong&gt; = You cook the full meal using what&amp;rsquo;s already on your workbench.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional attention = you run back to the warehouse for every spoon of spice 😅&lt;br&gt;
FlashAttention = bring the whole spice rack once, cook in peace! 👨‍🍳&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-summary&#34;&gt;✅ Summary&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Traditional Attention&lt;/th&gt;
&lt;th&gt;FlashAttention&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Memory Access&lt;/td&gt;
&lt;td&gt;Frequent HBM access&lt;/td&gt;
&lt;td&gt;One-time block transfer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SRAM Usage&lt;/td&gt;
&lt;td&gt;Under-utilized&lt;/td&gt;
&lt;td&gt;Fully utilized per block&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Computation Location&lt;/td&gt;
&lt;td&gt;Mix of HBM and registers&lt;/td&gt;
&lt;td&gt;All in SRAM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speed&lt;/td&gt;
&lt;td&gt;Slower, memory bottleneck&lt;/td&gt;
&lt;td&gt;Much faster, memory-efficient&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;FlashAttention is a key breakthrough for making large models faster and more scalable — especially during inference.&lt;/p&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Attacks on Large Language Models (LLMs)</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</link>
      <pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Large Language Models (LLMs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Adversarial attacks on large language models (LLMs) involve manipulating inputs to deceive the model into generating harmful, biased, or incorrect outputs. These attacks exploit the vulnerabilities of LLMs, which rely on patterns in training data to generate responses. Below is an overview of key concepts, types of attacks, implications, and defense strategies.&lt;/p&gt;














&lt;figure  id=&#34;figure-an-overview-of-threats-to-llm-based-applications-source-lillog-blog&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An overview of threats to LLM-based applications (source: Lil&amp;#39;Log Blog)&#34; srcset=&#34;
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png 400w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_39113d4dab022780f9de83ba4af61166.png 760w,
               /post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/adversarial-attacks/threats-overview_hu5cc59e8e6448994ce2a12d24e267d6f5_401678_213025f04731a3fd00a3d61c5d51a00f.png&#34;
               width=&#34;760&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An overview of threats to LLM-based applications (source: Lil&amp;rsquo;Log Blog)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-types-of-adversarial-attacks-on-llms&#34;&gt;&lt;strong&gt;1. Types of Adversarial Attacks on LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-evasion-attacks&#34;&gt;a. Evasion Attacks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Attackers modify input text (e.g., by altering words, punctuation, or structure) to trick the model into producing unintended outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding a few words like &amp;ldquo;I am a helpful assistant&amp;rdquo; to a prompt to manipulate the model&amp;rsquo;s response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Can lead to misinformation, phishing, or generation of harmful content.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-poisoning-attacks&#34;&gt;&lt;strong&gt;b. Poisoning Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Corrupting training data to influence the model&amp;rsquo;s behavior. Attackers inject malicious examples during training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Including biased or harmful data in the training set to make the model generate toxic responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Long-term degradation of model reliability and trustworthiness.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-injection-attacks&#34;&gt;&lt;strong&gt;c. Injection Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inserting malicious code or prompts into the input to alter the model&amp;rsquo;s execution flow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Using adversarial prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; to exploit the model&amp;rsquo;s tendency to follow instructions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Enables exploitation of model capabilities for malicious purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-data-poisoning&#34;&gt;&lt;strong&gt;d. Data Poisoning&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Similar to poisoning attacks but focuses on corrupting the training dataset to bias the model&amp;rsquo;s outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Adding fake user interactions that encourage the model to generate harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Systemic bias and ethical risks in model behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-model-inversion-attacks&#34;&gt;&lt;strong&gt;e. Model Inversion Attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Inferring sensitive information about the model&amp;rsquo;s training data by analyzing outputs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Reverse-engineering the model to reveal private data or patterns in the training set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Privacy breaches and exposure of proprietary information.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-implications-of-adversarial-attacks&#34;&gt;&lt;strong&gt;2. Implications of Adversarial Attacks&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security Risks&lt;/strong&gt;: Phishing, misinformation, and malware generation via manipulated prompts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ethical Concerns&lt;/strong&gt;: Reinforcement of biases, hate speech, or harmful content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trust Erosion&lt;/strong&gt;: Users may lose confidence in LLMs for critical tasks like healthcare, finance, or legal advice.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operational Disruption&lt;/strong&gt;: Attackers could disrupt services by causing models to fail or produce incorrect outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-defense-mechanisms&#34;&gt;&lt;strong&gt;3. Defense Mechanisms&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;a-adversarial-training&#34;&gt;&lt;strong&gt;a. Adversarial Training&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models on adversarial examples to improve robustness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Introduce perturbed inputs during training to make the model resistant to attacks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires access to adversarial examples, which may be difficult to generate for LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-input-sanitization&#34;&gt;&lt;strong&gt;b. Input Sanitization&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Detect and filter malicious patterns in inputs (e.g., using regex or keyword matching).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Blocking suspicious prompts like &amp;ldquo;Generate a phishing email&amp;rdquo; or &amp;ldquo;I am a helpful assistant.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May fail against sophisticated, subtle attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;c-model-ensembles&#34;&gt;&lt;strong&gt;c. Model Ensembles&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Use multiple models to cross-validate outputs and detect inconsistencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If one model generates a harmful response, others may flag it as anomalous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Increases computational overhead and complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;d-uncertainty-estimation&#34;&gt;&lt;strong&gt;d. Uncertainty Estimation&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Train models to estimate confidence in their outputs, flagging uncertain responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: If the model is unsure about a prompt, it may refuse to generate a response.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Requires careful calibration and may reduce usability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;e-prompt-engineering-defenses&#34;&gt;&lt;strong&gt;e. Prompt Engineering Defenses&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: Design prompts to resist adversarial manipulation (e.g., using multi-step reasoning or safety checks).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Incorporating safety constraints like &amp;ldquo;Avoid harmful content&amp;rdquo; into the prompt.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: May not fully prevent attacks, especially if the adversary tailors prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-research-and-tools&#34;&gt;&lt;strong&gt;4. Research and Tools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Key Papers&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Adversarial Examples for Neural Network Language Models&amp;rdquo;&lt;/strong&gt; (Emti et al.) – Explores adversarial examples in NLP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Prompt Injection Attacks on Language Models&amp;rdquo;&lt;/strong&gt; (Zhang et al.) – Demonstrates how prompts can be weaponized.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;Defending Against Prompt Injection Attacks&amp;rdquo;&lt;/strong&gt; (Li et al.) – Proposes defenses against adversarial prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adversarial Text Generation Tools&lt;/strong&gt;: Generate adversarial examples for testing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Auditing Frameworks&lt;/strong&gt;: Analyze model behavior for biases or vulnerabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-challenges-and-future-directions&#34;&gt;&lt;strong&gt;5. Challenges and Future Directions&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Nature of Attacks&lt;/strong&gt;: Adversaries continuously evolve techniques, requiring ongoing research.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balancing Safety and Usability&lt;/strong&gt;: Defenses must avoid overly restrictive measures that hinder model functionality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Domain Collaboration&lt;/strong&gt;: Combining insights from cybersecurity, NLP, and ethics to address risks holistically.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Adversarial attacks on LLMs pose significant risks to security, ethics, and trust. While defenses like adversarial training and input sanitization offer partial protection, the dynamic nature of these threats demands continuous innovation. Researchers and practitioners must prioritize robustness, transparency, and ethical considerations to ensure the safe deployment of LLMs in real-world applications.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GLiNER: A Generalist Model for Named Entity Recognition using Bidirectional Transformers</title>
      <link>https://leminhnguyen.github.io/post/nlp-research/gliner/</link>
      <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/nlp-research/gliner/</guid>
      <description>&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: 20px&#34;&gt;
&lt;h3 id=&#34;1-what-is-named-entity-recognition-ner&#34;&gt;1. What is Named Entity Recognition (NER)?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; is a fundamental task in Natural Language Processing (NLP) that involves &lt;strong&gt;identifying and classifying spans of text&lt;/strong&gt; that refer to real-world entities such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Persons&lt;/strong&gt; (e.g., &amp;ldquo;Albert Einstein&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organizations&lt;/strong&gt; (e.g., &amp;ldquo;United Nations&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Locations&lt;/strong&gt; (e.g., &amp;ldquo;Paris&amp;rdquo;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dates, Products, Diseases&lt;/strong&gt;, and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional NER systems are trained on a &lt;strong&gt;fixed set of entity types&lt;/strong&gt;, which limits their adaptability to new domains or tasks. Recently, &lt;strong&gt;Open NER&lt;/strong&gt; has emerged as a flexible paradigm that allows recognizing arbitrary entity types based on natural language instructions — a direction GLiNER directly embraces and enhances.&lt;/p&gt;
&lt;h3 id=&#34;2-overview&#34;&gt;2. Overview&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GLiNER&lt;/strong&gt; is a compact and general-purpose model for &lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt; that leverages &lt;strong&gt;Bidirectional Transformers&lt;/strong&gt; (like BERT or DeBERTa) to extract arbitrary types of entities from text — without being constrained to a fixed label set. Unlike traditional NER models or large language models (LLMs) like ChatGPT, GLiNER is &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;efficient&lt;/strong&gt;, and designed for &lt;strong&gt;zero-shot generalization&lt;/strong&gt; across domains and languages.&lt;/p&gt;
&lt;p&gt;Traditional NER systems are limited by a fixed ontology of entity types. While LLMs (e.g., GPT-3, ChatGPT) allow open-type NER via prompting, they are &lt;strong&gt;computationally expensive&lt;/strong&gt;, &lt;strong&gt;slow (token-by-token decoding)&lt;/strong&gt;, and often impractical in production due to API cost and latency. GLiNER aims to:&lt;/p&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px; margin-top: -15px; margin-bottom: -15px&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Retain the &lt;strong&gt;flexibility of LLMs&lt;/strong&gt; in handling arbitrary entity types.&lt;/li&gt;
&lt;li&gt;Achieve &lt;strong&gt;high performance&lt;/strong&gt; with &lt;strong&gt;orders of magnitude fewer parameters&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Enable &lt;strong&gt;parallel extraction&lt;/strong&gt; of entities rather than autoregressive generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id=&#34;3-model-architecture&#34;&gt;3. Model Architecture&lt;/h3&gt;
&lt;p&gt;GLiNER reframes NER as a &lt;strong&gt;semantic matching problem&lt;/strong&gt; between entity types and text spans in a &lt;strong&gt;shared latent space&lt;/strong&gt;.














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png 400w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_b31f86d47083376b16f5847cefd3ac4d.png 760w,
               /post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/GLiNER_hu3eb41bb17ef7a5fa1483b85584c09001_149421_7285fd65ba49efb805259c48efae741d.png&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Input Format&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ENT] person [ENT] organization [ENT] location [SEP] Text...
- `[ENT]`: special token preceding each entity type.
- `[SEP]`: separates entity types from input text.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bidirectional Encoder&lt;/strong&gt;: A BiLM (e.g., DeBERTa-v3) encodes both entity types and the input text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span Representation Module&lt;/strong&gt;: Computes span embeddings from token representations using a feedforward network:
\[
S_{ij} = \text{FFN}(h_i \oplus h_j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity Representation Module&lt;/strong&gt;: Processes entity type embeddings via another FFN.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Matching Layer&lt;/strong&gt;: Calculates matching score:
\[
\phi(i, j, t) = \sigma(S_{ij}^T q_t)
\]
where $\sigma$ is the sigmoid function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Strategy&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Binary cross-entropy loss over span/type pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Source&lt;/strong&gt;: Trained on &lt;strong&gt;Pile-NER&lt;/strong&gt;, a dataset derived from The Pile corpus with 44.8k passages and 13k entity types. Labels were generated by &lt;strong&gt;ChatGPT&lt;/strong&gt;, acting as a &lt;em&gt;teacher model&lt;/em&gt; (data-level distillation).

    
    
    
    
    
    
    
    
    
    
    
    
    
    &lt;figure  id=&#34;figure-prompting-chatgpt-for-entity-extraction-in-pile-ner-dataset&#34;&gt;
      &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
        &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Prompting ChatGPT for entity extraction in Pile-NER dataset&#34; srcset=&#34;
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png 400w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_78fed6eb2a2853a0c989d1c62aa41340.png 760w,
                   /post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_1200x1200_fit_lanczos_3.png 1200w&#34;
                   src=&#34;https://leminhnguyen.github.io/post/nlp-research/gliner/pile-NER_hu8b503a12746f28e4bf404f716c021685_36796_4bd7893016f128d68bc35b7ded00b273.png&#34;
                   width=&#34;749&#34;
                   height=&#34;265&#34;
                   loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
      &lt;/div&gt;&lt;figcaption&gt;
          Prompting ChatGPT for entity extraction in Pile-NER dataset
        &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Techniques for robustness&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;Negative sampling of entity types.&lt;/li&gt;
&lt;li&gt;Random shuffling and dropping of entity prompts.&lt;/li&gt;
&lt;li&gt;Span length cap (max 12 tokens) for efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-experimental-results&#34;&gt;4. Experimental Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For zero-shot evaluation, on 20 diverse NER benchmarks and out-of-domain (OOD) tasks, &lt;code&gt;GLiNER-L (0.3B)&lt;/code&gt; outperforms: ChatGPT, InstructUIE (11B), UniNER (13B) and even GoLLIE (7B) in most cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For multilingual performance without multilingual training: &lt;code&gt;GLiNER-Multi (mDeBERTa)&lt;/code&gt; surpasses ChatGPT on 8 out of 11 languages (e.g., Spanish, German, Russian). which shows &lt;strong&gt;strong generalization&lt;/strong&gt;, even on unseen scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With supervised fine-tuning, after fine-tuning on labeled datasets, GLiNER competes closely with or surpasses InstructUIE, performs nearly as well as UniNER (larger LLaMA-based model). Pretraining on Pile-NER improves data efficiency, especially with small datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiency and Scalability&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GLiNER allows &lt;strong&gt;parallel inference&lt;/strong&gt; for multiple entity types.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Time&lt;/strong&gt;: ~5 hours on a single A100 GPU for GLiNER-L.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parameter Sizes&lt;/strong&gt;: 50M (S), 90M (M), 300M (L), compared to 7B–13B in baselines.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-ablation-insights&#34;&gt;5. Ablation Insights&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Negative sampling (50%)&lt;/td&gt;
&lt;td&gt;Best F1 balance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dropping entity types&lt;/td&gt;
&lt;td&gt;+1.4 F1 on OOD datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deBERTa-v3 backbone&lt;/td&gt;
&lt;td&gt;Outperforms RoBERTa, BERT, ALBERT, ELECTRA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Earlier NER approaches include rule-based systems, sequence labeling (e.g., BiLSTM-CRF), and span classification.&lt;/li&gt;
&lt;li&gt;LLM-based models (e.g., InstructUIE, UniNER) use instruction-tuning or generation.&lt;/li&gt;
&lt;li&gt;GLiNER offers a &lt;strong&gt;middle ground&lt;/strong&gt;: lightweight yet capable of open-type NER.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h3&gt;
&lt;p&gt;GLiNER is a &lt;strong&gt;generalist, scalable, and high-performing model&lt;/strong&gt; for Named Entity Recognition that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bridges the gap between classic NER and large LLM-based models.&lt;/li&gt;
&lt;li&gt;Achieves &lt;strong&gt;state-of-the-art zero-shot results&lt;/strong&gt; with minimal resources.&lt;/li&gt;
&lt;li&gt;Demonstrates &lt;strong&gt;robust multilingual and cross-domain generalization&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes it an excellent candidate for real-world NER applications in &lt;strong&gt;low-resource, high-efficiency environments&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;🔗 &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/urchade/GLiNER&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/urchade/GLiNER&lt;/a&gt;&lt;br&gt;
📄 &lt;strong&gt;Paper&lt;/strong&gt;: Urchade Zaratiana et al., &lt;em&gt;GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer&lt;/em&gt;, arXiv:2311.08526&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Vietnamese Voice Conversion</title>
      <link>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/</link>
      <pubDate>Sat, 09 Mar 2024 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;div style=&#34;text-align:justify&#34;&gt;
This thesis develops a voice conversion model for Vietnamese based on the Phoneme Hallucinator model with 2 adoptions: (1) Add a Text2SSL module to get more context information before performing the KNN algorithm, (2) To create a more
diverse dataset we apply spectrogram-resize (SR) based data augmentation idea from Free-VC model which distorts speaker information without changing content information to generate more ”speakers”.
&lt;/div&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-the-proposal-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The proposal model&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_402d308e4cc0466a6716761c67ac5260.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_5bbc6f5994e960a7413518bb425da12a.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/new-vc-architecture_hu971822d0f98b31a6e6ccd103a428207c_122016_402d308e4cc0466a6716761c67ac5260.png&#34;
               width=&#34;760&#34;
               height=&#34;607&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The proposal model
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_54a6cd26a3842fb84b50dc5e8f9b4ac5.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_8f518ab3170d7857360ba1ffa07379d5.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/spec-augmentation_hud7d16e2cc8f6180373396b214284b457_491908_54a6cd26a3842fb84b50dc5e8f9b4ac5.png&#34;
               width=&#34;760&#34;
               height=&#34;351&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;div id=&#39;section-1&#39; class=&#39;section&#39; style=&#34;width: 100%&#34;&gt;
        &lt;h2&gt;Comparing different methods&lt;/h2&gt;
        &lt;p&gt;This section compares the baseline and the proposal model.&lt;/p&gt;
&lt;table style=&#34;width: 100%&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Source&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Target&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Baseline Model&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Proposal Model&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[trangntt]&lt;/b&gt; Female to Female Conversion &lt;td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[trangntt] &lt;/b&gt; Male to Female Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[nguyenlm] &lt;/b&gt; Male to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[2] phone_hallucinator_mn.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[nguyenlm] &lt;/b&gt; Female to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[3] phone_hallucinator.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[thanhpv] &lt;/b&gt; Male to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] source.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] target.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] knn-vc.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/demo/[6] phone_hallucinator_hr.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
      &lt;tr&gt;
      &lt;td colspan=&#34;4&#34;&gt; &lt;b&gt;[thanhpv] &lt;/b&gt; Female to Male Conversion  &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;table&#34; style=&#34;width: 100%&#34;&gt;
&lt;/div&gt;
&lt;!-- ![Python](images/python.svg) --&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_ba247c54d0c340584a86934861cb6f46.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-10-04-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png&#34;
               width=&#34;720&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;</description>
    </item>
    
    <item>
      <title>Comparing batch vs layer normalization</title>
      <link>https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/</guid>
      <description>&lt;h2 id=&#34;-batch-vs-layer-normalization&#34;&gt;💣 Batch vs Layer Normalization&lt;/h2&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
The purpose of normalization is to provide &lt;span style=&#39;font-weight:bold&#39;&gt; an uniform scale for the input data &lt;/span&gt; to avoid varing in huge range. The normalization method ensures there is no loss of information and even the range of values isn&#39;t affected. In spite of normalizing the input data, &lt;span style=&#39;font-weight:bold&#39;&gt; the value of activations of certain neurons in the hidden layers can start varying across a wide scale during the training process. &lt;/span&gt; This means the input to the neurons to the next hidden layer will also range across the wide range, bringing instability.














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_26e8274f2bf3d60894ddf4d1cdbfe1ca.webp 400w,
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_caee8622773b04039cc2c664010e6c40.webp 760w,
               /post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_1200x1200_fit_q75_h2_lanczos_2.webp 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/tts-learns/tts-learns/images/batch-vs-layer-normalization_hubc5419aa4d7ca81a5c5c39fc33adf9bc_24068_26e8274f2bf3d60894ddf4d1cdbfe1ca.webp&#34;
               width=&#34;760&#34;
               height=&#34;444&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Batch Normalization Layer is applied for neural networks where the training is done in mini-batches. We divide the data into batches with a certain batch size and then pass it through the network. Batch normalization is applied on the neuron activation for all the samples in the mini-batch such that the mean of output lies close to 0 and the standard deviation lies close to 1. It also introduces two learning parameters gama and beta in its calculation which are all optimized during training. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Layer Normalization which addresses the drawbacks of batch normalization. This technique is not dependent on batches and the normalization is applied on the neuron for a single instance across all features. Here also mean activation remains close to 0 and mean standard deviation remains close to 1. &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span style=&#39;font-weight:bold; font-size: 18px;&#39;&gt; The key difference &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch Normalization depends on mini-batch size and may not work properly for smaller batch sizes. On the other hand, Layer normalization does not depend on mini-batch size.&lt;/li&gt;
&lt;li&gt;In batch normalization, input values of the same neuron for all the data in the mini-batch are normalized. Whereas in layer normalization, input values for all neurons in the same layer are normalized for each data sample.&lt;/li&gt;
&lt;li&gt;Batch normalization works better with fully connected layers and convolutional neural network (CNN) but it shows poor results with recurrent neural network (RNN). On the other hand, the main advantage of Layer normalization is that it works really well with RNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningknowledge.ai/keras-normalization-layers-explained-for-beginners-batch-normalization-vs-layer-normalization/#:~:text=Batch%20Normalization%20vs%20Layer%20Normalization,-Before%20wrapping%20up&amp;text=In%20batch%20normalization%2C%20input%20values,normalized%20for%20each%20data%20sample.&#34; style=&#34;text-align: justify; font-size: 15px;&#34;&gt;Batch-vs-Layer-Normalization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fix &#34;[Errno 32] Broken pipe&#34; in Python</title>
      <link>https://leminhnguyen.github.io/post/linux-learns/broken-pipe-error/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/linux-learns/broken-pipe-error/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;p&gt;One day, I&amp;rsquo;ve tried to run a python script using multiprocessing technique &lt;code&gt;n_jobs=10&lt;/code&gt; and for a while the program crashed and raised the &lt;strong&gt;[Errno 32] Broken pipe&lt;/strong&gt; error.
With some google searches I founded the problem as well the solution for it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Broken pipe&amp;rdquo; is essentially an IOError error (short for input/output error), which happened at the Linux system level. It usually occurs when reading and writing files, or in other words, doing file input/output or network input/output (via sockets) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In programs that uses worker processes to speed up processing and make use of multi-core CPUs, you can try reducing the number of the worker processes to see whether the error disappear or not &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From that suggestion, I reduced &lt;code&gt;n_jobs=10&lt;/code&gt; to &lt;code&gt;n_jobs=5&lt;/code&gt; and boom, the error got disappeared.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://linuxpip.org/broken-pipe-python-error&#34; style=&#34;text-align: justify; font-size: 15px;&#34;&gt;[Errno 32] Broken pipe in Python&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to kill zombie processes using GPU ?</title>
      <link>https://leminhnguyen.github.io/post/linux-learns/kill-zombie-processes/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/linux-learns/kill-zombie-processes/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;!-- ![Python](images/python.svg) --&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/linux-learns/kill-zombie-processes/images/zombie_hu6d97b3e824db4a9840663df6fd482ef2_449021_e550662dba03fb1e0cbb4c95ac466ad0.png 400w,
               /post/linux-learns/kill-zombie-processes/images/zombie_hu6d97b3e824db4a9840663df6fd482ef2_449021_21b82c453c043986e8d659acaa888025.png 760w,
               /post/linux-learns/kill-zombie-processes/images/zombie_hu6d97b3e824db4a9840663df6fd482ef2_449021_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/linux-learns/kill-zombie-processes/images/zombie_hu6d97b3e824db4a9840663df6fd482ef2_449021_e550662dba03fb1e0cbb4c95ac466ad0.png&#34;
               width=&#34;760&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;h3 id=&#34;what-is-a-zombie-process&#34;&gt;What is a zombie process?&lt;/h3&gt;
&lt;p&gt;As you know, in Linux OS when we start an application the OS will create a process and this process can start other processes. The process starts other processes is refered as the parent and the new processes are refered as the children. The Linux OS keeps the information of processes in a table called the process table. The parent and the children run almost independently, but sometimes they share some resources (input, output) or contexts. When a child finished its job, it will send a &lt;code&gt;SIGCHLD&lt;/code&gt; signal to the parent. The parent then reads the exit code of the child and removes its entry from the process table, this also cleans the resources used by the child. But there are sometimes the children cannot send the &lt;code&gt;SIGCHLD&lt;/code&gt; signal to the parent or the parent was died by incident, in such cases the children outlive from their parent and the Linux OS refers them as &lt;code&gt;orphaned&lt;/code&gt; or &lt;code&gt;zombie&lt;/code&gt; processes.&lt;/p&gt;
&lt;h3 id=&#34;the-problem-of-zombie-processes&#34;&gt;The problem of zombie processes&lt;/h3&gt;
&lt;p&gt;Because of outliving the parent, the resources used by the children (&lt;code&gt;zombie&lt;/code&gt;) cannot be released, and hence, other processes cannot use these resources. To overcome this problem, we need to kill the children manually based on their ids. But, the main question is how we can find the the ids of the children? To answer that question, let&amp;rsquo;s continue to the next sections.&lt;/p&gt;
&lt;h3 id=&#34;killing-zombie-processes-using-gpu&#34;&gt;Killing zombie processes using GPU&lt;/h3&gt;
&lt;p&gt;Working as a research engineer, I&amp;rsquo;m usually using &lt;code&gt;GPUs&lt;/code&gt; to train the deep learning models and checking the used resources with &lt;code&gt;nvtop&lt;/code&gt; command. Usually each process using &lt;code&gt;GPU&lt;/code&gt; will has an entry in the &lt;code&gt;nvtop&lt;/code&gt; table and the Linux kernel refers that process as the parent process, the entry consists of some information about that process, for example, &lt;code&gt;PID&lt;/code&gt; - the parent id, &lt;code&gt;USER&lt;/code&gt; - the user that the parent belongs to, &lt;code&gt;GPU&lt;/code&gt; - the GPU id used by the parent&amp;hellip;














&lt;figure  id=&#34;figure-nvtop_snapshot&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/linux-learns/kill-zombie-processes/images/nvtop_snapshot_hu80f2f16fbe58c54e81357197183d5f64_349386_fa9a822110acef161bbdfe1115f146bc.png 400w,
               /post/linux-learns/kill-zombie-processes/images/nvtop_snapshot_hu80f2f16fbe58c54e81357197183d5f64_349386_8f85c6b05b4cf30cd4b59c2d3da45a42.png 760w,
               /post/linux-learns/kill-zombie-processes/images/nvtop_snapshot_hu80f2f16fbe58c54e81357197183d5f64_349386_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/linux-learns/kill-zombie-processes/images/nvtop_snapshot_hu80f2f16fbe58c54e81357197183d5f64_349386_fa9a822110acef161bbdfe1115f146bc.png&#34;
               width=&#34;760&#34;
               height=&#34;515&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
To kill a process using &lt;code&gt;GPU&lt;/code&gt; we simply use the command &lt;code&gt;kill PID&lt;/code&gt; or &lt;code&gt;kill -9 PID&lt;/code&gt;, but there are some cases we cannot kill the process by that way, for example, the process has &lt;code&gt;PID=18309&lt;/code&gt; in the &lt;a href=&#34;#figure-nvtop_snapshot&#34;&gt;figure1&lt;/a&gt;. This because the process (&lt;code&gt;parent&lt;/code&gt;) is already dead (indicated by &lt;code&gt;N/A&lt;/code&gt; USER column) but the children (&lt;code&gt;orphaned&lt;/code&gt;) are still alive and hold the resouces (in this case, the zombie proceses are holding about 85% GPU MEM). In order to access the child processes you have to excute &lt;code&gt;sudo fuser -v /dev/nvidia*&lt;/code&gt; and all processes using GPUs will be listed with each &lt;code&gt;GPU&lt;/code&gt; id. For example, when running the &lt;code&gt;sudo fuser -v /dev/nvidia*&lt;/code&gt; command on my training server we will see the output looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo fuser -v /dev/nvidia*
                     USER        PID ACCESS COMMAND
/dev/nvidia0:        nguyenlm  15909 F.... nvtop
                     nguyenlm  20717 F.... nvtop
                     nguyenlm  21042 F.... nvtop
                     root      24536 F.... nvtop
                     nguyenlm  24787 F...m tensorboard
                     nguyenlm  25078 F...m python
                     nguyenlm  25079 F...m python
                     nguyenlm  25080 F...m python
                     nguyenlm  25081 F...m python
                     nguyenlm  25082 F...m python
                     nguyenlm  25085 F...m python
                     nguyenlm  32199 F...m python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we have a dozen of processes using &lt;code&gt;GPU=0&lt;/code&gt; (&lt;em&gt;python, nvtop, tensorboard&lt;/em&gt;). Simply, we can kill them all with their &lt;code&gt;PIDs&lt;/code&gt; by the &lt;code&gt;kill&lt;/code&gt; command as mentioned to release the resources. However, we can do that easier by an observation, the zombie processes are usually have consecutive ids, so if we look the output carefully we will see a group of processes has the id ranged from &lt;code&gt;25078&lt;/code&gt; to &lt;code&gt;25082&lt;/code&gt; and those actually are zombie &lt;code&gt;PIDs&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://theblackcat102.github.io/fixing-nvidia-gpu-zombie-process/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kill zombie process using GPU memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://realpython.com/python-subprocess/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The subprocess Module: Wrapping Programs With Python&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>KNN-VC vs Phoneme Hallucinator [09/03/2024] ?</title>
      <link>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-09-03-2024/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-09-03-2024/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/phoneme_hallucinator_hu247d5b7fd5c49ebd144b6f04e075f068_52595_25a7f069e8f68b7c0d925051fcbe7fb4.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/phoneme_hallucinator_hu247d5b7fd5c49ebd144b6f04e075f068_52595_ba4aaaf560428797c0d9482aa234b0a6.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/phoneme_hallucinator_hu247d5b7fd5c49ebd144b6f04e075f068_52595_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/phoneme_hallucinator_hu247d5b7fd5c49ebd144b6f04e075f068_52595_25a7f069e8f68b7c0d925051fcbe7fb4.png&#34;
               width=&#34;752&#34;
               height=&#34;486&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;div id=&#39;section-1&#39; class=&#39;section&#39; style=&#34;width: 100%&#34;&gt;
        &lt;h2&gt;Comparing different methods&lt;/h2&gt;
        &lt;p&gt;This section compares Phoneme Hallucinator kNN-VC and Phoneme Hallucinator.&lt;/p&gt;
&lt;table style=&#34;width: 100%&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Source&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Target&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;kNN-VC&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Phoneme Hallucinator&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_knn-vc_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;table&#34; style=&#34;width: 100%&#34;&gt;
&lt;/div&gt;
&lt;!-- ![Python](images/python.svg) --&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_ba247c54d0c340584a86934861cb6f46.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-09-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png&#34;
               width=&#34;720&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;</description>
    </item>
    
    <item>
      <title>KNN-VC vs Phoneme Hallucinator [23/03/2024] ?</title>
      <link>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-23-03-2024/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-23-03-2024/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/voice-conversion-improvement_hu9c2c2c0b1c61256ebe168fc02f8de765_124214_a2d27f9f56804762a78877b0a2bfc74d.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/voice-conversion-improvement_hu9c2c2c0b1c61256ebe168fc02f8de765_124214_83c259e7f182431e085e9b327cdd2c18.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/voice-conversion-improvement_hu9c2c2c0b1c61256ebe168fc02f8de765_124214_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/voice-conversion-improvement_hu9c2c2c0b1c61256ebe168fc02f8de765_124214_a2d27f9f56804762a78877b0a2bfc74d.png&#34;
               width=&#34;760&#34;
               height=&#34;446&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;div id=&#39;section-1&#39; class=&#39;section&#39; style=&#34;width: 100%&#34;&gt;
        &lt;h2&gt;Comparing different methods&lt;/h2&gt;
        &lt;p&gt;This section compares Phoneme Hallucinator kNN-VC and Phoneme Hallucinator.&lt;/p&gt;
&lt;table style=&#34;width: 100%&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Source&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Target&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Phoneme Hallucinator&lt;/th&gt;
      &lt;th style=&#34;min-width: 175px&#34;&gt;Phoneme Hallucinator + Text2SSL&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_01(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_02.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_03.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_04.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_05(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/source_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/target_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;div class=&#39;labeled-audio&#39;&gt;&lt;audio preload=&#39;metadata&#39; controls&gt;&lt;source src=&#39;samples/result_phone-hallucinator_06(text2ssl)_.wav&#39; type=&#39;audio/mpeg&#39;&gt;&lt;/audio&gt;&lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;table&#34; style=&#34;width: 100%&#34;&gt;
&lt;/div&gt;
&lt;!-- ![Python](images/python.svg) --&gt;
&lt;!-- 













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png 400w,
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_ba247c54d0c340584a86934861cb6f46.png 760w,
               /post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/voice-conversion/voice-conversion-23-03-2024/images/main_huec624690a5e35421dcb4a187a8905990_505572_3102e5b3320686e4bfd436129d57b956.png&#34;
               width=&#34;720&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt; --&gt;</description>
    </item>
    
    <item>
      <title>Postnet Layer</title>
      <link>https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/</link>
      <pubDate>Wed, 09 Mar 2022 11:01:17 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/</guid>
      <description>&lt;h2 id=&#34;-postnet-layer&#34;&gt;💣 Postnet Layer&lt;/h2&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
In some end-to-end TTS models today, after the hidden representations are passed through the decoder we got the mel-spectrogram which contains the predictions of the acoustic features. Finally, the decoder predictions are passed over &lt;span style=&#39;font-weight:bold&#39;&gt; the Postnet layer which predicts residual information to improve the construction performance of the model &lt;/span&gt;. The section below notes some insights about &lt;span style=&#39;font-weight:bold&#39;&gt; the Postnet layer &lt;/span&gt; by me when learning TTS.
&lt;/div&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1908.11535.pdf&#34; style=&#34;text-align: justify; font-size: 20px;&#34;&gt; 
1. https://arxiv.org/pdf/1908.11535.pdf - 30 Aug 2019
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;In addition to the decoder, some systems have a post-net,
an additional network that predicts acoustic features. A post-net was originally introduced to convert acoustic features to different acoustic features that were suitable for an adopted waveform synthesis method, for example, from mel spectrograms to linear spectrograms [2] or mel spectrograms to vocoder parameters [4]. In recent studies the role of the post-net was to improve the acoustic features predicted by the decoder to improve quality further [5, 6]. The post-net introduces an additional loss term in the objective function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: justify; font-size: 15px;&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2008.03388.pdf&#34; style=&#34;text-align: justify; font-size: 20px;&#34;&gt; 2. https://arxiv.org/pdf/2008.03388.pdf - 11 Aug 2020&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Relative to DAR, C-DAR has three additional changes that
do not significantly impact naturalness or controllability, but
provide additional insights into F0 generation. First, a 5-layer
postnet [3] follows the autoregressive RNN. We find that this
postnet has the effect of reducing autoregressive sampling errors and tightening the posterior distribution around the argmax
(Figure 2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_3d454b8539ed4f3c7668c209efbf7722.png 400w,
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_59f0b4c02707bb74d1a3fa8996c51df0.png 760w,
               /post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://leminhnguyen.github.io/post/speech-research/tts-learns/postnet-layer/images/postnet_hu55b7b326dc86b48b5eb6c9cddaea2438_98725_3d454b8539ed4f3c7668c209efbf7722.png&#34;
               width=&#34;760&#34;
               height=&#34;432&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Talk: Unittesting for Data Science</title>
      <link>https://leminhnguyen.github.io/publication/talk-unittesting-for-data-science/</link>
      <pubDate>Tue, 18 Jan 2022 10:09:35 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/publication/talk-unittesting-for-data-science/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://leminhnguyen.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://leminhnguyen.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://leminhnguyen.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
